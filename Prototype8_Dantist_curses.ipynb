{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitrijLeontev/Dmitrij_Leontev/blob/main/Prototype8_Dantist_curses.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pljzw91IiAD0"
      },
      "source": [
        "Подтягивание базы знаний с Github,\n",
        "\n",
        "вмонтирован  промптsistem,\n",
        "\n",
        "организован подход с обращением к ключю OpenAI и токену Githab через секретные ключи Google collab,\n",
        "\n",
        "подсчет токенов для каждого фрагмента и построение графика,\n",
        "\n",
        "суммаризация диалога,\n",
        "\n",
        "отображение стоимости вопрос + промпт + ответ и\n",
        "\n",
        "суммы за весь диалог.\n",
        "\n",
        "для выхода из режима диалога с консультантом наберите  в окне диалоговом - stop"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Наша задача учесть еще и промпт.\n",
        "\n",
        "Из документации OpenAI берем прайс на токены:\n",
        "\n",
        "стоимость 1M input tokens: $0.15(входящие токены)/1Млн- вопрос\n",
        "\n",
        "стоимость 1M input tokens: $0.15(входящие токены)/1Млн- промпт и  ближайшие 6 подтянутых чанков\n",
        "\n",
        "стоимость 1M output tokens: $0.6(исходящие токены)/1Млн- ответ.\n",
        "\n",
        "Консультант получает вопрос от пользователя и одновременно получает промпт sistem как инструкцию и на основе этого формирует ответ.\n",
        "\n",
        "1. Получается, что мы должны учесть цену за вопрос(input tokens)+ цену за токены  промпта(input tokens):\n",
        "Сумма за вопрос и промт  =($0.15/1000000)* n токенов вопроса = + ($0.15/1000000)* n токенов 6 подтянутых чанков = $0.00000015* n токенов вопроса+$0.00000015* n  токенов промпта + $0.00000015* n токенов подтянутых 6 ближайших чанков\n",
        "\n",
        "2. Ответ идет уже без промпта, так как уже при подаче вопроса происходит обучение по промпту. Консультант принял вопрос и промпт и отвечает. В ответе считается число токенов ответа :\n",
        "\n",
        "Цена в долларах за токен ответа: $0.6/1000000= $0.0000006\n",
        "\n",
        "3. Учитывая пункты 1. и 2. мы получаем сумму за сессию вопрос, ответ, промпт:\n",
        "\n",
        "($0.15/1000000)* n токенов вопроса + ($0.15/1000000)* n токенов промпта систем + ($0.15/1000000)* n токенов 6 ближайших подтянутых чанков+ ($0.6/1000000)* n токенов ответа.\n",
        "\n",
        "n - количество токенов\n",
        "\n",
        "4. В конце надо сложить все суммы за все сессии вопрос+ промпт+ответ+цена подтянутых 6 чанков, ближайших, в каждой сессии и мы получим сумму за весь диалог.\n",
        "\n",
        "Приступим."
      ],
      "metadata": {
        "id": "ullM4umKLfyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBjC37DE795K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "868cc676-2bde-40b5-d74d-08beafa99b4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.4/225.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m798.0/798.0 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m500.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain_openai==0.0.2 faiss-cpu==1.7.4 openai==1.6.1 tiktoken==0.5.2 langchain_community==0.0.11 langchain==0.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WjgYv3-HZNk",
        "outputId": "2afc478f-b64f-428e-a5de-1f0071210286"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyGithub\n",
            "  Downloading PyGithub-2.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pynacl>=1.4.0 (from PyGithub)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: requests>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.32.3)\n",
            "Requirement already satisfied: pyjwt>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from PyGithub) (2.0.7)\n",
            "Collecting Deprecated (from PyGithub)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: cryptography>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (43.0.0)\n",
            "Requirement already satisfied: cffi>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from pynacl>=1.4.0->PyGithub) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.14.0->PyGithub) (2024.7.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from Deprecated->PyGithub) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.22)\n",
            "Downloading PyGithub-2.4.0-py3-none-any.whl (362 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.6/362.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Installing collected packages: Deprecated, pynacl, PyGithub\n",
            "Successfully installed Deprecated-1.2.14 PyGithub-2.4.0 pynacl-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install PyGithub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKc3lJmA8E7O"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import re\n",
        "import requests\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from github import Github\n",
        "import tiktoken\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kVvAvNO8c9Q"
      },
      "outputs": [],
      "source": [
        "# Установка API ключа OpenAI из секретных данных Colab\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
        "\n",
        "# Создание клиента OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "# Установка GitHub API ключа\n",
        "github_token = userdata.get('GITHUB_TOKEN')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ynGNwlnnnD"
      },
      "source": [
        "Добавил расчет стоимости диалога >>>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Средняя стоимость токенов\n",
        "INPUT_TOKEN_COST = 0.15 / 1000000 # USD за 1000000 входящих токенов\n",
        "OUTPUT_TOKEN_COST = 0.6 / 1000000  # USD за 1000000 исходящих токенов\n",
        "\n",
        "def calculate_cost(question_token_count: int, prompt_token_count: int, answer_token_count: int) -> float:\n",
        "    \"\"\"\n",
        "    Функция для расчета стоимости на основе количества токенов.\n",
        "    \"\"\"\n",
        "    question_cost = (question_token_count + prompt_token_count) * INPUT_TOKEN_COST\n",
        "    answer_cost = answer_token_count * OUTPUT_TOKEN_COST\n",
        "    return question_cost + answer_cost\n"
      ],
      "metadata": {
        "id": "KANMcuF_CXWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bWpIOf3KmrK"
      },
      "outputs": [],
      "source": [
        "# функция для загрузки документа по ссылке из гугл драйв\n",
        "def load_document_text(url: str) -> str:\n",
        "    # Extract the document ID from the URL\n",
        "    match_ = re.search('/document/d/([a-zA-Z0-9-_]+)', url)\n",
        "    if match_ is None:\n",
        "        raise ValueError('Invalid Google Docs URL')\n",
        "    doc_id = match_.group(1)\n",
        "\n",
        "    # Download the document as plain text\n",
        "    response = requests.get(f'https://docs.google.com/document/d/{doc_id}/export?format=txt')\n",
        "    response.raise_for_status()\n",
        "    text = response.text\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noxT8hqnmvHE"
      },
      "source": [
        "Блок отвечающий за подтягивание базы знаний с Github\n",
        "\n",
        "1) нужно на место многоточия вставить свой токен Github\n",
        "\n",
        "2) Загрузка данных из GitHub: Функция load_document_from_github загружает текстовый файл из GitHub, используя токен доступа.\n",
        "\n",
        "Проверка данных: Проверяется, удалось ли загрузить данные, и выводятся первые 1000 символов для проверки.\n",
        "\n",
        "Обработка текста: Текст разбивается на чанки с помощью MarkdownHeaderTextSplitter.\n",
        "\n",
        "Подсчет токенов: Функция num_tokens_from_string подсчитывает количество токенов в каждом чанке.\n",
        "\n",
        "Построение графика: Строится гистограмма распределения длин чанков в токенах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIR9q2nIoqaM"
      },
      "outputs": [],
      "source": [
        "# Получение токена из секретных данных Colab\n",
        "github_token = userdata.get('GITHUB_TOKEN')\n",
        "\n",
        "# Проверка на наличие токена\n",
        "if not github_token:\n",
        "    raise ValueError(\"GitHub token is missing. Please set it in the Colab secrets.\")\n",
        "\n",
        "# Загрузка документа с GitHub\n",
        "def load_document_from_github(url: str, github_token: str) -> str:\n",
        "    match = re.match(r\"https://github.com/([^/]+)/([^/]+)/blob/([^/]+)/(.*)\", url)\n",
        "    if not match:\n",
        "        raise ValueError(\"Invalid GitHub URL\")\n",
        "    username, repo_name, branch, file_path = match.groups()\n",
        "\n",
        "    g = Github(github_token)\n",
        "    repo = g.get_repo(f\"{username}/{repo_name}\")\n",
        "    file_content = repo.get_contents(file_path, ref=branch)\n",
        "    return file_content.decoded_content.decode('utf-8')\n",
        "\n",
        "# Загрузка базы знаний из GitHub\n",
        "github_url = 'https://github.com/NeuronsUII....' # здесь заменить на ссылку на базу знаний с Github\n",
        "\n",
        "try:\n",
        "    data_from_github = load_document_from_github(github_url, github_token)\n",
        "    print(data_from_github[:1000])  # Вывод первых 1000 символов документа для проверки\n",
        "except Exception as e:\n",
        "    print(f\"Error loading document: {e}\")\n",
        "\n",
        "# Убедимся, что текст был загружен\n",
        "if 'data_from_github' in locals():\n",
        "    # Разделение текста на чанки\n",
        "    splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"#\", \"Header 1\")])\n",
        "    fragments = splitter.split_text(data_from_github)\n",
        "\n",
        "    # Функция для подсчета токенов\n",
        "    def num_tokens_from_string(text: str, model_name: str) -> int:\n",
        "        encoding = tiktoken.get_encoding(model_name)\n",
        "        tokens = encoding.encode(text)\n",
        "        return len(tokens)\n",
        "\n",
        "    # Проверка, что каждый фрагмент является строкой\n",
        "    fragments = [fragment if isinstance(fragment, str) else str(fragment) for fragment in fragments]\n",
        "\n",
        "    fragment_token_counts = [num_tokens_from_string(fragment, \"cl100k_base\") for fragment in fragments]\n",
        "\n",
        "    # Построение графика распределения длин токенов\n",
        "    plt.hist(fragment_token_counts, bins=50, alpha=0.5, label='Fragments')\n",
        "    plt.title('Распределение длин чанков в токенах')\n",
        "    plt.xlabel('Token Count')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Ошибка: данные не были загружены из GitHub.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM1koliTPcZK"
      },
      "source": [
        "бдлок загрузки базы знаний с Github закончен"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkE1n0dy8ysx"
      },
      "outputs": [],
      "source": [
        "# Вывод первых 1000 символов документа для проверки\n",
        "print(data_from_github[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TcNOqvm9B5x"
      },
      "outputs": [],
      "source": [
        "# Указание заголовков, по которым будет происходить разделение\n",
        "headers_to_split_on = [\n",
        "    (\"#\", \"Header 1\"),\n",
        "]\n",
        "\n",
        "# Создание объекта MarkdownHeaderTextSplitter для разделения текста\n",
        "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# Разделение текста на чанки\n",
        "chunks = splitter.split_text(data_from_github)\n",
        "\n",
        "# Вывод чанков\n",
        "for i, chunk in enumerate(chunks):\n",
        "    print(f\"Чанк {i+1}:\\n{chunk}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWAd7R7t9I4B"
      },
      "outputs": [],
      "source": [
        "# Инструкция в system\n",
        "\n",
        "system = load_document_text('https://docs.google.com/document/d/1EEqTk........') # заполняем параметр ссылкой на составленный промпт. Не забываем открыть доступ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RR8cj4b9SfE",
        "outputId": "5bf4ec60-e818-494f-9c5f-820ba66ba231"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKw_M3pE9NM6"
      },
      "outputs": [],
      "source": [
        "# Вывод первых 4000 символов system\n",
        "print(system[:4000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjtknHM--QRj"
      },
      "outputs": [],
      "source": [
        "# выводим несколько чанков, чтобы убедиться, что все получилось в необходимом нам формате\n",
        "chunks[0:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvGCVGQn-iEB",
        "outputId": "bc42229b-3b7d-43df-b053-2d13e0f91b4b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "len(chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVybHR5b9SKZ"
      },
      "outputs": [],
      "source": [
        "# Инициализируем модель эмбеддингов\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Создадим индексную базу из разделенных фрагментов текста\n",
        "db = FAISS.from_documents(chunks, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdqaxjn39Zyw"
      },
      "source": [
        "Отдельный блок для сохранения базы Faiss (если необходимо)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DiWXXsr-ssW"
      },
      "outputs": [],
      "source": [
        "# Задаем имя и путь для сохранения файла\n",
        "folder_path = '/content/drive/MyDrive/'\n",
        "index_name = \"db\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLjKBETV-0Zr"
      },
      "outputs": [],
      "source": [
        "# Сохраняем db на ваш Google Drive\n",
        "db.save_local(folder_path=folder_path, index_name=index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiqEv8f--5hb"
      },
      "outputs": [],
      "source": [
        "# Путь к сохраненному файлу\n",
        "saved_file_path = f\"{folder_path}{index_name}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M87rkypa_Cch",
        "outputId": "b13d2c0f-f349-411c-d539-3e7af46303da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Файл сохранен в /content/drive/MyDrive/db. Перейдите на Google Drive, найдите файл и настройте общий доступ.\n"
          ]
        }
      ],
      "source": [
        "# Настройка общего доступа и получение ссылки вручную\n",
        "print(f\"Файл сохранен в {saved_file_path}. Перейдите на Google Drive, найдите файл и настройте общий доступ.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXdiPjNO_XQq"
      },
      "outputs": [],
      "source": [
        "# Вывод ссылки для ручного копирования и настройки общего доступа\n",
        "drive_link = f\"https://drive.google.com/drive/u/0/my-drive\"\n",
        "print(f\"Ссылка на ваш Google Drive: {drive_link}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtXs-dkh_jPD"
      },
      "source": [
        "Конец блока сохранения базы Faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFyoqe6KrR0R"
      },
      "outputs": [],
      "source": [
        "def insert_newlines(text: str, max_len: int = 170) -> str:\n",
        "    \"\"\"\n",
        "    Функция разбивает длинный текст на строки определенной максимальной длины.\n",
        "    \"\"\"\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        if len(current_line + \" \" + word) > max_len:\n",
        "            lines.append(current_line)\n",
        "            current_line = \"\"\n",
        "        current_line += \" \" + word\n",
        "    lines.append(current_line)\n",
        "    return \"\\n\".join(lines)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_index(system, topic, search_index, verbose=1):\n",
        "    \"\"\"\n",
        "    Функция для ответа на вопросы с использованием индекса и расчета стоимости.\n",
        "\n",
        "    Args:\n",
        "      system: Промпт системы\n",
        "      topic: Вопрос пользователя\n",
        "      search_index: Индекс базы знаний\n",
        "      verbose: Флаг для вывода отладочной информации\n",
        "\n",
        "    Returns:\n",
        "      answer: Ответ на вопрос\n",
        "      cost: Общая стоимость в USD\n",
        "      question_token_count: Количество токенов вопроса\n",
        "      prompt_token_count: Количество токенов промпта\n",
        "      answer_token_count: Количество токенов ответа\n",
        "    \"\"\"\n",
        "    # Поиск релевантных отрезков из базы знаний\n",
        "    docs = search_index.similarity_search(topic, k=6)\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "\n",
        "    message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\nОтрывок документа №{i+1}\\n=====================' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "    if verbose: print('message_content :\\n ======================================== \\n', message_content)\n",
        "\n",
        "    client = OpenAI()\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system},\n",
        "        {\"role\": \"user\", \"content\": f\"Ответь на вопрос. Документ с информацией для ответа: {message_content}\\n\\nВопрос пользователя: \\n{topic}\"}\n",
        "    ]\n",
        "\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini-2024-07-18\",\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    answer = completion.choices[0].message.content\n",
        "\n",
        "    # Подсчет токенов\n",
        "    question_token_count = num_tokens_from_string(topic, \"cl100k_base\")\n",
        "    prompt_token_count = num_tokens_from_string(system, \"cl100k_base\")\n",
        "    answer_token_count = num_tokens_from_string(answer, \"cl100k_base\")\n",
        "    cost = calculate_cost(question_token_count, prompt_token_count, answer_token_count)\n",
        "\n",
        "    return answer, cost, question_token_count, prompt_token_count, answer_token_count\n"
      ],
      "metadata": {
        "id": "OkNVEMvhC2YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_questions(dialog):\n",
        "    \"\"\"\n",
        "    Функция возвращает саммаризированный текст диалога.\n",
        "    \"\"\"\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"Ты - нейро-саммаризатор. Твоя задача - саммаризировать диалог, который тебе пришел. Если пользователь назвал свое имя, обязательно отрази его в саммаризированном диалоге\"},\n",
        "        {\"role\": \"user\", \"content\": \"Саммаризируй следующий диалог консультанта и пользователя: \" + \" \".join(dialog)}\n",
        "    ]\n",
        "\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini-2024-07-18\",\n",
        "        messages=messages,\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    return completion.choices[0].message.content\n"
      ],
      "metadata": {
        "id": "fWR4WaG0DDzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_user_question_dialog(system, db, user_question, question_history):\n",
        "    \"\"\"\n",
        "    Функция возвращает ответ на вопрос пользователя с расчетом стоимости.\n",
        "    \"\"\"\n",
        "    summarized_history = \"\"\n",
        "    if len(question_history) > 0:\n",
        "        # Извлекаем только вопросы и ответы для саммаризации\n",
        "        summarized_history = \"Вот саммаризированный предыдущий диалог с пользователем: \" + \\\n",
        "                              summarize_questions([q + ' ' + (a if a else '') for q, a, *_ in question_history])\n",
        "\n",
        "    topic = summarized_history + \" Актуальный вопрос пользователя: \" + user_question\n",
        "\n",
        "    # Получаем ответ и стоимость\n",
        "    answer, cost, question_token_count, prompt_token_count, answer_token_count = answer_index(system, topic, db)\n",
        "\n",
        "    question_history.append((user_question, answer if answer else '', cost, question_token_count, prompt_token_count, answer_token_count))\n",
        "\n",
        "    # Выводим саммаризированный текст, который видит модель\n",
        "    if summarized_history:\n",
        "        print('****************************')\n",
        "        print(summarized_history)\n",
        "        print('****************************')\n",
        "\n",
        "    print(f\"Стоимость текущего вопроса и ответа: ${cost:.4f}\")\n",
        "    print(f\"Количество токенов - Вопрос: {question_token_count}, Промпт: {prompt_token_count}, Ответ: {answer_token_count}\")\n",
        "\n",
        "    return answer, cost\n"
      ],
      "metadata": {
        "id": "z6-dobNUDLtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_dialog(system, db):\n",
        "    \"\"\"\n",
        "    Функция запускает диалог между пользователем и нейро-консультантом.\n",
        "    \"\"\"\n",
        "    question_history = []\n",
        "    total_cost = 0.0\n",
        "\n",
        "    while True:\n",
        "        user_question = input('Пользователь: ')\n",
        "        if user_question.lower() == 'stop':\n",
        "            break\n",
        "        answer, cost = answer_user_question_dialog(system, db, user_question, question_history)\n",
        "        total_cost += cost\n",
        "        print('Консультант:', insert_newlines(answer))\n",
        "\n",
        "    print(f\"Общая стоимость диалога: ${total_cost:.2f}\")\n",
        "    return\n"
      ],
      "metadata": {
        "id": "XsehRmroDTxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Провеям работу нейро-консультанта на нескольких самостоятельно сгенерированных вопросах.\n",
        "# После запуска, вводите ваши вопросы в консоли. Для завершения сеанса диалога, введите stop.\n",
        "run_dialog(system, db)"
      ],
      "metadata": {
        "id": "QGqS89Fdclvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вывод:\n",
        "\n",
        "консультант работает и считает каждый вопрос - ответ с учетом промпта подаваемого с вопросом. Также консультант считает по итогу сумму за весь диалог. Контекст на моих вопросах держит хорошо. Суммаризацию диалога показывает.\n",
        "\n",
        "Следующий шаг: Создание модели(функции) , которая будет сохранять ответы и вопросы и подтянутые чанки в отдельную базу знаний, затем при поступлении вопроса будет искать ответ в этой базе знаний. Если ответ не найден , то она ищет ответ в основной базе знаний. И  так продолжается пока диалог не закончится."
      ],
      "metadata": {
        "id": "Y2WJU_-Q-Bu5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNmJOqDVEt4ZDTevIQ8qlXo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}