{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitrijLeontev/Dmitrij_Leontev/blob/main/DzDatascience/%D0%94%D0%97_UltraPro_20_%D0%BF%D0%B8%D1%81%D0%B0%D1%82%D0%B5%D0%BB%D0%B5%D0%B9_%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2_%D1%81_%D0%BF%D0%BE%D0%BC%D0%BE%D1%89%D1%8C%D1%8E_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D1%8B%D1%85_%D1%81%D0%B5%D1%82%D0%B5%D0%B9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hahJ-ONkfmR"
      },
      "source": [
        "Дорогой студент!\n",
        "\n",
        "В домашнем задании Ultra Pro занятия по обработке тектсов с помощью НС мы ставим задачу распознать уже не 6, как ранее, а целых 20 русских писателей! Это подразумевает и больший размер базы для обучения соответственно. Ячейка для скачивания базы уже включена в ноутбук задания.\n",
        "\n",
        "\n",
        " В задании необходимо выполнить следующие пункты:\n",
        "\n",
        "  1. Загрузить саму базу по ссылке и подговить файлы базы для обработки.\n",
        "  2. Создать обучающую и проверочную выборки, обратив особое внимание на балансировку базы: количество примеров каждого класса должно быть примерно одного порядка. При этом для разбивки необходимо применить цикл. Проверочная выборка должна быть 20% от общей выборки.\n",
        "  3. Подготовьте выборки для обучения и обучите сеть. Добейтесь результата точности сети не менее 95% на проверочной выборке модели Bag of Words и 75-80% - для модели Embedding.\n",
        "   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QolTxRtFWm3r"
      },
      "source": [
        "## Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBys9ajOW43u",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c6ad18b-9f4f-4a1b-a24d-c1b646681f35"
      },
      "source": [
        "import gdown                                      # Подключим функцию gdown\n",
        "gdown.download('https://storage.yandexcloud.net/aiueducation/Content/base/l7/20writers.zip', None, quiet=True)      # Скачивание файла"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'20writers.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Работа с массивами данных\n",
        "import numpy as np\n",
        "\n",
        "# Функции-утилиты для работы с категориальными данными\n",
        "from tensorflow.keras import utils\n",
        "\n",
        "# Класс для конструирования последовательной модели нейронной сети\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Основные слои\n",
        "from tensorflow.keras.layers import Dense, Dropout, SpatialDropout1D, BatchNormalization, Embedding, Flatten, Activation\n",
        "\n",
        "# Токенизатор для преобразование текстов в последовательности\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Матрица ошибок классификатора\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Функции операционной системы\n",
        "import os\n",
        "\n",
        "# Работа со временем\n",
        "import time\n",
        "\n",
        "# Регулярные выражения\n",
        "import re\n",
        "\n",
        "# Запись в файлы и чтение из файлов структур данных Python\n",
        "import pickle\n",
        "\n",
        "# Отрисовка графиков\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "_KHjCpU-_3Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Команды, именуемые «магическими» предназначены для выполнения кода непосредственно в консоли или ячейке вашего ноутбука. Они начинаются с символа восклицательного знака – !.\n",
        "\n",
        "Далее !unzip распаковывает архив в папку с именем указанным после ключа -d (в нашем случае папка writers)\n",
        "\n",
        "С помощью команды !ls мы выводим содержимое папки (`writers') на экран\n",
        "\n",
        "Теперь все файлы для работы в вашем распоряжении!"
      ],
      "metadata": {
        "id": "QBJrkpWuACE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Распаковка архива в папку 20writers\n",
        "!unzip -qo 20writers.zip -d 20writers/"
      ],
      "metadata": {
        "id": "SrB1YHihih3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Просмотр содержимого папки\n",
        "!ls 20writers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLaLhXk2uegE",
        "outputId": "94ef037e-8e87-49bb-c277-7f1406a2f3b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Беляев.txt    Гончаров.txt     Каверин.txt    Лесков.txt     Толстой.txt\n",
            "Булгаков.txt  Горький.txt      Катаев.txt     Носов.txt      Тургенев.txt\n",
            "Васильев.txt  Грибоедов.txt    Куприн.txt     Пастернак.txt  Чехов.txt\n",
            "Гоголь.txt    Достоевский.txt  Лермонтов.txt  Пушкин.txt     Шолохов.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Важно! Если при распаковке файлов вы видите кракозябры вместо названий, необходимо указать кодировку:\n",
        "\n",
        "!unzip -qo -O UTF-8 writers.zip -d writers/\n",
        "\n",
        "Все три переменных ниже содержат названия строчного типа:\n",
        "\n",
        "Первая – название папки с файлами текстов. Вторая – слово «обучающая» в названии файла (для поиска в дальнейшем). Третья – слово «тестовая» в названии файла (также для поиска)."
      ],
      "metadata": {
        "id": "lHI89PQjAOuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Настройка констант для загрузки данных\n",
        "FILE_DIR  = '20writers'                     # Папка с текстовыми файлами\n",
        "SIG_TRAIN = 'обучающая'                   # Признак обучающей выборки в имени файла\n",
        "SIG_TEST  = 'тестовая'                    # Признак тестовой выборки в имени файла\n",
        "\n"
      ],
      "metadata": {
        "id": "P_O0XrcvAQPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Первые три переменных содержат в себе по пустому списку, что вы видите по пустым квадратным скобкам – [ ].\n",
        "\n",
        "● CLASS_LIST – в этот список вы поместите имя каждого класса и в итоге получите единый список с именами всех задействованных писателей;\n",
        "\n",
        "● text_train – в этой переменной в результате будет список, содержащий одну единственную строку: непрерывный текст для обучения сети;\n",
        "\n",
        "● text_test – аналогично text_train, но с назначением текста для тестирования точности сети.\n",
        "\n",
        "Первый цикл for просто проходится по всем файлам с текстами в общей папке.\n",
        "\n",
        "Команда re.match относится к разряду регулярных выражений, которые вы изучите отдельно, поскольку на первый взгляд они могут казаться весьма сложными.\n",
        "\n",
        "Здесь re.match ищет совпадения в названиях файлов для деления по классам:\n",
        "\n",
        "● class_name – это имя каждого писателя в названии файла.\n",
        "\n",
        "● subset_name – наименование выборки (обучающая или тестовая).\n",
        "\n",
        "re.match выполнит сортировку всех файлов. При этом данный метод ориентируется на строку '((.+)) (\\S+)_' на первой позиции в своих параметрах. Это условно кодированные команды для работы с текстом и деления, которое как раз описано выше. Пока что примите этот параметр как данность, впоследствии вы сможете изучить его при желании.\n",
        "\n",
        "Переменная cls сохраняет в себе список имен всех писателей в виде индексов, а первая и единственная в этом блоке команда print() информирует о распределении файлов по выборкам и создании списка с именами писателей.\n",
        "\n",
        "Команда-менеджер with удобна для работы с потоком файлов, в вашем случае – с текстами писателей. Она гарантированно сократит количество строк для нужной операции до минимального, а также всегда поможет избежать возможных ошибок кода, которые могли бы случиться при использовании другого способа.\n",
        "\n",
        "В блоке ниже with открывает каждый текстовый файл в папке и записывает весь текст в единую строку, сохраненную в переменной text. В переменной subset закреплена проверка принадлежности текста либо к обучающей, либо к проверочной выборке, и сразу после добавляет каждый текст к соответствующей выборке, разделив слова текста только пробелами.\n",
        "\n",
        "len(CLASS_LIST) – сохраняет в переменной CLASS_COUNT общее количество писателей (их 20)."
      ],
      "metadata": {
        "id": "NXyyAvA-Aer9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1XpQBYoVKdy",
        "outputId": "9ba481d6-2100-426a-c39c-b7d98d3d409f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка датасета. Добавляются имена классов и соответствующие тексты.\n",
        "# Все тексты преобразуются в строку и объединяются для каждого класса и выборки\n",
        "CLASS_LIST = []\n",
        "text_train = []\n",
        "text_test = []\n",
        "\n",
        "for file_name in os.listdir(FILE_DIR):\n",
        "    # Выделение имени класса и типа выборки из имени файла\n",
        "    m = re.match('\\((.+)\\) (\\S+)_', file_name)\n",
        "    # Если выделение получилось, то файл обрабатывается\n",
        "    if m:\n",
        "        class_name = m[1]\n",
        "        subset_name = m[2].lower()\n",
        "        # Проверка типа выборки в имени файла\n",
        "        is_train = SIG_TRAIN in subset_name\n",
        "        is_test = SIG_TEST in subset_name\n",
        "\n",
        "        # Если тип выборки обучающая либо тестовая - файл обрабатывается\n",
        "        if is_train or is_test:\n",
        "            # Добавление нового класса, если его еще нет в списке\n",
        "            if class_name not in CLASS_LIST:\n",
        "                print(f'Добавление класса \"{class_name}\"')\n",
        "                CLASS_LIST.append(class_name)\n",
        "                # Инициализация соответствующих классу строк текста\n",
        "                text_train.append('')\n",
        "                text_test.append('')\n",
        "\n",
        "            # Поиск индекса класса для добавления содержимого файла в выборку\n",
        "            cls = CLASS_LIST.index(class_name)\n",
        "            print(f'Добавление файла \"{file_name}\" в класс \"{CLASS_LIST[cls]}\", {subset_name} выборка.')\n",
        "            with open(f'{FILE_DIR}/{file_name}', 'r') as f:\n",
        "                # Загрузка содержимого файла в строку\n",
        "                text = f.read()\n",
        "            # Определение выборки, куда будет добавлено содержимое\n",
        "            subset = text_train if is_train else text_test\n",
        "            # Добавление текста к соответствующей выборке класса. Концы строк заменяются на пробел\n",
        "            subset[cls] += ' ' + text.replace('\\n', ' ')\n",
        "\n",
        "# Определение количества классов\n",
        "CLASS_COUNT = len(CLASS_LIST)"
      ],
      "metadata": {
        "id": "l2m0195Pxx6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Прочитанные классы текстов\n",
        "print(CLASS_LIST)\n",
        "\n",
        "# Количество текстов в обучающей выборке\n",
        "print(len(text_train))\n",
        "\n",
        "# Количество символов в одном из текстов обучающей выборки\n",
        "print(len(text_train[3]))"
      ],
      "metadata": {
        "id": "SrmcvzV0yXqE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "27eed2c4-7ce7-4bad-d3b4-56398ff60146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5e16d83ab7a6>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Количество символов в одном из текстов обучающей выборки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверим, были ли ошибки при загрузке текстов\n",
        "for i, class_name in enumerate(CLASS_LIST):\n",
        "    print(f\"Количество текстов в обучающей выборке для класса '{class_name}': {len(text_train[i])}\")\n",
        "\n",
        "# Проверим ошибки при загрузке текстов\n",
        "for i, class_name in enumerate(CLASS_LIST):\n",
        "    print(f\"Количество текстов в проверочной выборке для класса '{class_name}': {len(text_test[i])}\")\n",
        "\n",
        "# Проверим, были ли ошибки при загрузке текстов\n",
        "for i, class_name in enumerate(CLASS_LIST):\n",
        "    if len(text_train[i]) == 0:\n",
        "        print(f\"Ошибка при загрузке текстов для класса '{class_name}' в обучающей выборке\")\n",
        "    if len(text_test[i]) == 0:\n",
        "        print(f\"Ошибка при загрузке текстов для класса '{class_name}' в проверочной выборке\")\n"
      ],
      "metadata": {
        "id": "Mc7qQmBu5sat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Количество классов:\", len(CLASS_LIST))\n",
        "print(\"Количество текстов в обучающей выборке:\", len(text_train))\n",
        "print(\"Количество текстов в проверочной выборке:\", len(text_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CMP01IB55wc",
        "outputId": "046b63a5-feea-4a11-c076-e0ae1f33ef7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество классов: 0\n",
            "Количество текстов в обучающей выборке: 0\n",
            "Количество текстов в проверочной выборке: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка датасета. Добавляются имена классов и соответствующие тексты.\n",
        "CLASS_LIST = []\n",
        "text_train = []\n",
        "text_test = []\n",
        "\n",
        "for file_name in os.listdir(FILE_DIR):\n",
        "    # Выделение имени класса и типа выборки из имени файла\n",
        "    m = re.match('\\((.+)\\)_(\\S+)\\.txt', file_name)\n",
        "    # Если выделение получилось, то файл обрабатывается\n",
        "    if m:\n",
        "        class_name = m[1]\n",
        "        subset_name = m[2].lower()\n",
        "        # Проверка типа выборки в имени файла\n",
        "        is_train = SIG_TRAIN in subset_name\n",
        "        is_test = SIG_TEST in subset_name\n",
        "\n",
        "        # Если тип выборки обучающая либо тестовая - файл обрабатывается\n",
        "        if is_train or is_test:\n",
        "            # Добавление нового класса, если его еще нет в списке\n",
        "            if class_name not in CLASS_LIST:\n",
        "                print(f'Добавление класса \"{class_name}\"')\n",
        "                CLASS_LIST.append(class_name)\n",
        "                # Инициализация соответствующих классу строк текста\n",
        "                text_train.append('')\n",
        "                text_test.append('')\n",
        "\n",
        "            # Поиск индекса класса для добавления содержимого файла в выборку\n",
        "            cls = CLASS_LIST.index(class_name)\n",
        "            print(f'Добавление файла \"{file_name}\" в класс \"{CLASS_LIST[cls]}\", {subset_name} выборка.')\n",
        "            with open(f'{FILE_DIR}/{file_name}', 'r') as f:\n",
        "                # Загрузка содержимого файла в строку\n",
        "                text = f.read()\n",
        "            # Определение выборки, куда будет добавлено содержимое\n",
        "            subset = text_train if is_train else text_test\n",
        "            # Добавление текста к соответствующей выборке класса. Концы строк заменяются на пробел\n",
        "            subset[cls] += ' ' + text.replace('\\n', ' ')\n",
        "\n",
        "# Определение количества классов\n",
        "CLASS_COUNT = len(CLASS_LIST)\n"
      ],
      "metadata": {
        "id": "3s7JSTjJ6eCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте попробуем добавить больше отладочной информации, чтобы выявить, что именно происходит во время обработки файлов."
      ],
      "metadata": {
        "id": "nxzEPPgu6yFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка датасета. Добавляются имена классов и соответствующие тексты.\n",
        "CLASS_LIST = []\n",
        "text_train = []\n",
        "text_test = []\n",
        "\n",
        "for file_name in os.listdir(FILE_DIR):\n",
        "    # Выделение имени класса и типа выборки из имени файла\n",
        "    m = re.match('\\((.+)\\)_(\\S+)\\.txt', file_name)\n",
        "    # Вывод отладочной информации\n",
        "    print(f\"Имя файла: {file_name}, Результат выделения: {m}\")\n",
        "    # Если выделение получилось, то файл обрабатывается\n",
        "    if m:\n",
        "        class_name = m[1]\n",
        "        subset_name = m[2].lower()\n",
        "        # Вывод отладочной информации\n",
        "        print(f\"Имя класса: {class_name}, Тип выборки: {subset_name}\")\n",
        "        # Проверка типа выборки в имени файла\n",
        "        is_train = SIG_TRAIN in subset_name\n",
        "        is_test = SIG_TEST in subset_name\n",
        "        # Вывод отладочной информации\n",
        "        print(f\"Обучающая выборка: {is_train}, Тестовая выборка: {is_test}\")\n",
        "\n",
        "        # Если тип выборки обучающая либо тестовая - файл обрабатывается\n",
        "        if is_train or is_test:\n",
        "            # Добавление нового класса, если его еще нет в списке\n",
        "            if class_name not in CLASS_LIST:\n",
        "                print(f'Добавление класса \"{class_name}\"')\n",
        "                CLASS_LIST.append(class_name)\n",
        "                # Инициализация соответствующих классу строк текста\n",
        "                text_train.append('')\n",
        "                text_test.append('')\n",
        "\n",
        "            # Поиск индекса класса для добавления содержимого файла в выборку\n",
        "            cls = CLASS_LIST.index(class_name)\n",
        "            print(f'Добавление файла \"{file_name}\" в класс \"{CLASS_LIST[cls]}\", {subset_name} выборка.')\n",
        "            with open(f'{FILE_DIR}/{file_name}', 'r') as f:\n",
        "                # Загрузка содержимого файла в строку\n",
        "                text = f.read()\n",
        "            # Определение выборки, куда будет добавлено содержимое\n",
        "            subset = text_train if is_train else text_test\n",
        "            # Добавление текста к соответствующей выборке класса. Концы строк заменяются на пробел\n",
        "            subset[cls] += ' ' + text.replace('\\n', ' ')\n",
        "\n",
        "# Определение количества классов\n",
        "CLASS_COUNT = len(CLASS_LIST)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Mb1HU126zeU",
        "outputId": "4f8348a5-9fd4-4da3-f98d-54bc2bbf732d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Имя файла: Тургенев.txt, Результат выделения: None\n",
            "Имя файла: Носов.txt, Результат выделения: None\n",
            "Имя файла: Куприн.txt, Результат выделения: None\n",
            "Имя файла: Пастернак.txt, Результат выделения: None\n",
            "Имя файла: Шолохов.txt, Результат выделения: None\n",
            "Имя файла: Грибоедов.txt, Результат выделения: None\n",
            "Имя файла: Чехов.txt, Результат выделения: None\n",
            "Имя файла: Гончаров.txt, Результат выделения: None\n",
            "Имя файла: Беляев.txt, Результат выделения: None\n",
            "Имя файла: Катаев.txt, Результат выделения: None\n",
            "Имя файла: Толстой.txt, Результат выделения: None\n",
            "Имя файла: Горький.txt, Результат выделения: None\n",
            "Имя файла: Достоевский.txt, Результат выделения: None\n",
            "Имя файла: Каверин.txt, Результат выделения: None\n",
            "Имя файла: Гоголь.txt, Результат выделения: None\n",
            "Имя файла: Лесков.txt, Результат выделения: None\n",
            "Имя файла: Булгаков.txt, Результат выделения: None\n",
            "Имя файла: Пушкин.txt, Результат выделения: None\n",
            "Имя файла: Васильев.txt, Результат выделения: None\n",
            "Имя файла: Лермонтов.txt, Результат выделения: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте разделим каждый класс на обучающую и проверочную выборки. Мы можем использовать функцию train_test_split из библиотеки scikit-learn для этого. После разделения данных мы объединим их обратно в общие обучающие и проверочные выборки.\n",
        "\n",
        "Давайте начнем с примера кода для разделения данных."
      ],
      "metadata": {
        "id": "h0vmmfwc7k4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Подготовим списки для обучающих и проверочных данных\n",
        "X_train = []\n",
        "X_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "# Процентное соотношение между обучающей и проверочной выборками\n",
        "train_ratio = 0.8\n",
        "test_ratio = 1 - train_ratio\n",
        "\n",
        "for i, class_name in enumerate(CLASS_LIST):\n",
        "    # Разделение данных для текущего класса на обучающую и проверочную выборки\n",
        "    X_cls_train, X_cls_test = train_test_split(text_train[i], test_size=test_ratio, random_state=42)\n",
        "\n",
        "    # Добавление разделенных данных в списки обучающих и проверочных данных\n",
        "    X_train.extend(X_cls_train)\n",
        "    X_test.extend(X_cls_test)\n",
        "    y_train.extend([i] * len(X_cls_train))\n",
        "    y_test.extend([i] * len(X_cls_test))\n",
        "\n",
        "# Перемешивание данных\n",
        "combined_train_data = list(zip(X_train, y_train))\n",
        "combined_test_data = list(zip(X_test, y_test))\n",
        "import random\n",
        "random.shuffle(combined_train_data)\n",
        "random.shuffle(combined_test_data)\n",
        "X_train[:], y_train[:] = zip(*combined_train_data)\n",
        "X_test[:], y_test[:] = zip(*combined_test_data)\n",
        "\n",
        "# Проверка размеров обучающих и проверочных выборок\n",
        "print(\"Размер обучающей выборки:\", len(X_train))\n",
        "print(\"Размер проверочной выборки:\", len(X_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xy5h4Wn-7oOc",
        "outputId": "929b2ca0-2787-4282-94fa-af387a2faa7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 2, got 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-9b702f5ffa26>\u001b[0m in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcombined_train_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcombined_test_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Похоже, что в процессе объединения текстов с классами что-то пошло не так. Давайте внимательно изучим код и исправим эту проблему.\n",
        "\n",
        "Кажется, что списки text_train и text_test не содержат текстов для каждого класса. Давайте выведем их длины, чтобы убедиться, что данные загрузились правильно перед разделением на обучающую и проверочную выборки.\n",
        "\n",
        "Давайте проверим длины списков text_train и text_test, чтобы убедиться, что данные были загружены корректно."
      ],
      "metadata": {
        "id": "5QGGrlUJ72Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка длины списков text_train и text_test\n",
        "print(\"Длина списка text_train:\", len(text_train))\n",
        "print(\"Длина списка text_test:\", len(text_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pA-gJr-574AU",
        "outputId": "82b2fd82-1008-44c1-da87-eddb44170e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Длина списка text_train: 0\n",
            "Длина списка text_test: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Понятно, это объясняет, почему произошла ошибка. Как мы видим, списки text_train и text_test оказались пустыми, что означает, что данные не были загружены правильно.\n",
        "\n",
        "Давайте вернемся к коду, который загружает данные из файлов, и исправим проблему. Давайте убедимся, что регулярное выражение, которое мы использовали для извлечения имени класса и типа выборки из имен файлов, работает правильно. Для этого давайте снова выведем имена файлов и результаты выделения.\n",
        "\n",
        "Давайте проверим регулярное выражение, которое мы использовали для извлечения имени класса и типа выборки из имен файлов. Для этого давайте выведем имена файлов и результаты выделения, чтобы убедиться, что регулярное выражение работает правильно."
      ],
      "metadata": {
        "id": "-NP4392G8Qft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод имен файлов и результатов выделения\n",
        "for file_name in os.listdir(FILE_DIR):\n",
        "    # Выделение имени класса и типа выборки из имени файла\n",
        "    m = re.match('\\((.+)\\)_(\\S+)\\.txt', file_name)\n",
        "    print(f\"Имя файла: {file_name}, Результат выделения: {m}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FwbCKlW8R7L",
        "outputId": "fbb634e2-7e7c-4bc1-9e32-bf6d70f4bb58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Имя файла: Тургенев.txt, Результат выделения: None\n",
            "Имя файла: Носов.txt, Результат выделения: None\n",
            "Имя файла: Куприн.txt, Результат выделения: None\n",
            "Имя файла: Пастернак.txt, Результат выделения: None\n",
            "Имя файла: Шолохов.txt, Результат выделения: None\n",
            "Имя файла: Грибоедов.txt, Результат выделения: None\n",
            "Имя файла: Чехов.txt, Результат выделения: None\n",
            "Имя файла: Гончаров.txt, Результат выделения: None\n",
            "Имя файла: Беляев.txt, Результат выделения: None\n",
            "Имя файла: Катаев.txt, Результат выделения: None\n",
            "Имя файла: Толстой.txt, Результат выделения: None\n",
            "Имя файла: Горький.txt, Результат выделения: None\n",
            "Имя файла: Достоевский.txt, Результат выделения: None\n",
            "Имя файла: Каверин.txt, Результат выделения: None\n",
            "Имя файла: Гоголь.txt, Результат выделения: None\n",
            "Имя файла: Лесков.txt, Результат выделения: None\n",
            "Имя файла: Булгаков.txt, Результат выделения: None\n",
            "Имя файла: Пушкин.txt, Результат выделения: None\n",
            "Имя файла: Васильев.txt, Результат выделения: None\n",
            "Имя файла: Лермонтов.txt, Результат выделения: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте попробуем использовать только часть до расширения .txt в имени файла как имя класса. Для этого мы можем разделить имя файла по точке и взять первую часть."
      ],
      "metadata": {
        "id": "4F-4eBCq8oTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод имен файлов и результатов выделения\n",
        "for file_name in os.listdir(FILE_DIR):\n",
        "    # Выделение имени класса из имени файла\n",
        "    class_name = file_name.split('.')[0]\n",
        "    print(f\"Имя файла: {file_name}, Имя класса: {class_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6Ag2gLH8pbD",
        "outputId": "da5555c9-9085-46c0-9576-a6566d5c021f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Имя файла: Тургенев.txt, Имя класса: Тургенев\n",
            "Имя файла: Носов.txt, Имя класса: Носов\n",
            "Имя файла: Куприн.txt, Имя класса: Куприн\n",
            "Имя файла: Пастернак.txt, Имя класса: Пастернак\n",
            "Имя файла: Шолохов.txt, Имя класса: Шолохов\n",
            "Имя файла: Грибоедов.txt, Имя класса: Грибоедов\n",
            "Имя файла: Чехов.txt, Имя класса: Чехов\n",
            "Имя файла: Гончаров.txt, Имя класса: Гончаров\n",
            "Имя файла: Беляев.txt, Имя класса: Беляев\n",
            "Имя файла: Катаев.txt, Имя класса: Катаев\n",
            "Имя файла: Толстой.txt, Имя класса: Толстой\n",
            "Имя файла: Горький.txt, Имя класса: Горький\n",
            "Имя файла: Достоевский.txt, Имя класса: Достоевский\n",
            "Имя файла: Каверин.txt, Имя класса: Каверин\n",
            "Имя файла: Гоголь.txt, Имя класса: Гоголь\n",
            "Имя файла: Лесков.txt, Имя класса: Лесков\n",
            "Имя файла: Булгаков.txt, Имя класса: Булгаков\n",
            "Имя файла: Пушкин.txt, Имя класса: Пушкин\n",
            "Имя файла: Васильев.txt, Имя класса: Васильев\n",
            "Имя файла: Лермонтов.txt, Имя класса: Лермонтов\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Прекрасно! Теперь, когда у нас есть правильные имена классов, давайте модифицируем наш код для загрузки данных и их разделения на обучающую и проверочную выборки. Мы также объединим их обратно в один набор данных для каждой выборки."
      ],
      "metadata": {
        "id": "0ZIKPCGA9HSS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрите начало каждого текста из обучающей выборки:"
      ],
      "metadata": {
        "id": "EnY7kWd-A5yN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка загрузки: вывод начальных отрывков из каждого класса\n",
        "for cls in range(CLASS_COUNT):\n",
        "    print(f'Класс: {CLASS_LIST[cls]}')\n",
        "    print(f'  train: {text_train[cls][:200]}')\n",
        "    print(f'  test : {text_test[cls][:200]}')\n",
        "    print()"
      ],
      "metadata": {
        "id": "L0aS-HLHyeuc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "414a8c52-62a7-4d7c-f9fa-e27e93a0c66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'CLASS_COUNT' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a9b2ee6f98de>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Проверка загрузки: вывод начальных отрывков из каждого класса\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCLASS_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Класс: {CLASS_LIST[cls]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'  train: {text_train[cls][:200]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'  test : {text_test[cls][:200]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CLASS_COUNT' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь вы узнаете, как превратить текст из последовательного набора слов в последовательный набор чисел.\n",
        "\n",
        "Преобразование текстовых данных в числовые и векторные представления для обучения нейросети\n",
        "\n",
        "понадобится более объемный словарь.\n",
        "\n",
        "Это будет переменная VOCAB_SIZE, равная 20 000 индексам слов.\n",
        "\n",
        "Так вы выбрали общий размер словаря, а посредством WIN_SIZE = 1000 вы задаете размер каждой подвыборки внутри общей базы. Так сколько же тогда вы получите подвыборок?\n",
        "\n",
        "20 000 : 1000 = 20\n",
        "\n",
        "20 примеров для обучения!\n",
        "\n",
        "WIN_HOP = 100 – шаг или же попросту количество слов, на которое будет смещаться словарь от начала общего списка до самого конца, чтобы получить отличающиеся подвыборки."
      ],
      "metadata": {
        "id": "r2Km7AMSBDmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Задание параметров преобразования\n",
        "VOCAB_SIZE = 20000                        # Объем словаря для токенизатора\n",
        "WIN_SIZE   = 1000                         # Длина отрезка текста (окна) в словах\n",
        "WIN_HOP    = 100                          # Шаг окна разбиения текста на векторы"
      ],
      "metadata": {
        "id": "FFfQpuN7BFBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В блоке ниже создан класс timex, в основе которого лежит модуль для работы с реальным временем time.\n",
        "\n",
        "timex используют для измерения времени выполнения дальнейших операций.\n",
        "\n",
        "Важно: time.time() засекает текущее время, а {:.2f} в форматированной строке выводит время в виде десятичной дроби с двумя знаками (2f) после запятой."
      ],
      "metadata": {
        "id": "Ke3rcV7rBOEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Контекстный менеджер для измерения времени операций\n",
        "# Операция обертывается менеджером с помощью оператора with\n",
        "class timex:\n",
        "    def __enter__(self):\n",
        "        # Фиксация времени старта процесса\n",
        "        self.t = time.time()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, type, value, traceback):\n",
        "        # Вывод времени работы\n",
        "        print('Время обработки: {:.2f} с'.format(time.time() - self.t))"
      ],
      "metadata": {
        "id": "MwW28TJQBRts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее выполняется та же операция по формированию словаря частотности, только его объем уже равен значению VOCAB_SIZE, а это 20 000.\n",
        "\n",
        "Менеджер with поможет гладко обработать весь блок кода со словарем с помощью timex(). Так вы узнаете время обработки всего диапазона слов токенайзером. Учитывайте, что токенайзер еще не преобразует всю выборку сразу в одни лишь индексы (числовые значения), а формирует пары «слово: индекс»."
      ],
      "metadata": {
        "id": "NoHWu_NeBXym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизация и построение частотного словаря по обучающим текстам\n",
        "with timex():\n",
        "    # Используется встроенный в Keras токенизатор для разбиения текста и построения частотного словаря\n",
        "    tokenizer = Tokenizer(num_words=VOCAB_SIZE, filters='!\"#$%&()*+,-–—./…:;<=>?@[\\\\]^_`{|}~«»\\t\\n\\xa0\\ufeff',\n",
        "                          lower=True, split=' ', oov_token='неизвестное_слово', char_level=False)\n",
        "\n",
        "    # Использованы параметры:\n",
        "    # num_words   - объем словаря\n",
        "    # filters     - убираемые из текста ненужные символы\n",
        "    # lower       - приведение слов к нижнему регистру\n",
        "    # split       - разделитель слов\n",
        "    # char_level  - указание разделять по словам, а не по единичным символам\n",
        "    # oov_token   - токен для слов, которые не вошли в словарь\n",
        "\n",
        "    # Построение частотного словаря по обучающим текстам\n",
        "    tokenizer.fit_on_texts(text_train)\n",
        "\n",
        "    # Построение словаря в виде пар слово - индекс\n",
        "    items = list(tokenizer.word_index.items())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBv38rd5BY8s",
        "outputId": "b768845f-c086-46ea-8c07-fbcef90eb630"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Время обработки: 0.00 с\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод нескольких наиболее часто встречающихся слов\n",
        "print(items[:120])\n",
        "\n",
        "# Размер словаря может быть больше, чем num_words, но при преобразовании в последовательности\n",
        "# и векторы bag of words будут учтены только первые num_words слов\n",
        "print(\"Размер словаря\", len(items))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWRYOOtgBihc",
        "outputId": "70c9f1c9-a549-416e-f85d-5d09014b3905"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('неизвестное_слово', 1)]\n",
            "Размер словаря 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверка индекса слова в словаре\n",
        "try:\n",
        "    print('Интересующее слово имеет индекс:', tokenizer.word_index[input('Введите слово: ')])\n",
        "except:\n",
        "    print('Интересующего вас слова нет в словаре')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKp3ZmkiBoxU",
        "outputId": "83241c09-312a-4d4c-b32c-f57c90c11e59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введите слово: привет\n",
            "Интересующего вас слова нет в словаре\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "А далее вы как раз преобразуете текст в чистую последовательность индексов по частоте повторений слов. Вы это рассматривали немного раньше в занятии."
      ],
      "metadata": {
        "id": "S1VhVk3dBvLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Преобразование обучающих и проверочных текстов в последовательность индексов согласно частотному словарю\n",
        "with timex():\n",
        "    seq_train = tokenizer.texts_to_sequences(text_train)\n",
        "    seq_test = tokenizer.texts_to_sequences(text_test)\n",
        "\n",
        "    print(\"Фрагмент обучающего текста:\")\n",
        "    print(\"В виде оригинального текста:              \", text_train[1][:101])\n",
        "    print(\"Он же в виде последовательности индексов: \", seq_train[1][:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1GcOFwWBwX2",
        "outputId": "449cf231-e41e-429a-8197-627bb78dc97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Фрагмент обучающего текста:\n",
            "В виде оригинального текста:               []\n",
            "Он же в виде последовательности индексов:  []\n",
            "Время обработки: 0.00 с\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрите функцию вывода статистики по всем загруженным текстам – print_text_stats(), которая:\n",
        "\n",
        "Задает название выборки (параметр title – «обучающая» или «тестовая»). Отдельно проходит по каждому тексту (параметр texts). Проходит по сформированным последовательностям индексов слов (параметр sequences). Выводит количество символов каждого текста (счетчик переменной chars). Выводит количество слов каждого текста (счетчик переменной words). Суммирует все символы и слова в каждой выборке целиком (последняя строка в коде функции). Обратите внимание на вывод функции."
      ],
      "metadata": {
        "id": "ga_QKoo6COFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция вывода статистики по текстам\n",
        "def print_text_stats(title, texts, sequences, class_labels=CLASS_LIST):\n",
        "    # Суммарное количество символов и слов в тексте\n",
        "    chars = 0\n",
        "    words = 0\n",
        "\n",
        "    print(f'Статистика по {title} текстам:')\n",
        "\n",
        "    # Вывод итогов по всем классам данного набора текстов и их последовательностей индексов\n",
        "    for cls in range(len(class_labels)):\n",
        "        print('{:<15} {:9} символов,{:8} слов'.format(class_labels[cls],\n",
        "                                                      len(texts[cls]),\n",
        "                                                      len(sequences[cls])))\n",
        "        chars += len(texts[cls])\n",
        "        words += len(sequences[cls])\n",
        "\n",
        "    print('----')\n",
        "    print('{:<15} {:9} символов,{:8} слов\\n'.format('В сумме', chars, words))\n",
        "\n",
        "# Вывод итогов по текстам\n",
        "print_text_stats('обучающим', text_train, seq_train)\n",
        "print_text_stats('тестовым', text_test, seq_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8ocA4slCPZc",
        "outputId": "9e3957a5-d2ff-4222-8977-52b92fbee921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статистика по обучающим текстам:\n",
            "Пушкин                  0 символов,       0 слов\n",
            "Горький                 0 символов,       0 слов\n",
            "Каверин                 0 символов,       0 слов\n",
            "Катаев                  0 символов,       0 слов\n",
            "Гоголь                  0 символов,       0 слов\n",
            "Куприн                  0 символов,       0 слов\n",
            "Гончаров                0 символов,       0 слов\n",
            "Беляев                  0 символов,       0 слов\n",
            "Носов                   0 символов,       0 слов\n",
            "Чехов                   0 символов,       0 слов\n",
            "Шолохов                 0 символов,       0 слов\n",
            "Пастернак               0 символов,       0 слов\n",
            "Лесков                  0 символов,       0 слов\n",
            "Булгаков                0 символов,       0 слов\n",
            "Тургенев                0 символов,       0 слов\n",
            "Васильев                0 символов,       0 слов\n",
            "Грибоедов               0 символов,       0 слов\n",
            "Достоевский             0 символов,       0 слов\n",
            "Лермонтов               0 символов,       0 слов\n",
            "Толстой                 0 символов,       0 слов\n",
            "----\n",
            "В сумме                 0 символов,       0 слов\n",
            "\n",
            "Статистика по тестовым текстам:\n",
            "Пушкин                  0 символов,       0 слов\n",
            "Горький                 0 символов,       0 слов\n",
            "Каверин                 0 символов,       0 слов\n",
            "Катаев                  0 символов,       0 слов\n",
            "Гоголь                  0 символов,       0 слов\n",
            "Куприн                  0 символов,       0 слов\n",
            "Гончаров                0 символов,       0 слов\n",
            "Беляев                  0 символов,       0 слов\n",
            "Носов                   0 символов,       0 слов\n",
            "Чехов                   0 символов,       0 слов\n",
            "Шолохов                 0 символов,       0 слов\n",
            "Пастернак               0 символов,       0 слов\n",
            "Лесков                  0 символов,       0 слов\n",
            "Булгаков                0 символов,       0 слов\n",
            "Тургенев                0 символов,       0 слов\n",
            "Васильев                0 символов,       0 слов\n",
            "Грибоедов               0 символов,       0 слов\n",
            "Достоевский             0 символов,       0 слов\n",
            "Лермонтов               0 символов,       0 слов\n",
            "Толстой                 0 символов,       0 слов\n",
            "----\n",
            "В сумме                 0 символов,       0 слов\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание обучающей и проверочной выборок\n",
        "\n",
        "Функция split_sequence поможет поделить одну общую последовательность (параметр sequence) из индексов слов на то количество примеров, которые вы захотите (параметр win_size) со сдвигом по всей выборке, равному размеру шага (параметр hop). Вы делали то же с помощью вложенного в список цикла for, только теперь он задан как результат работы функции. Кто знает, сколько раз вам придется к этому прибегать, поэтому лучше сделать функцию!\n",
        "\n",
        "Есть и вторая функция – vectorize_sequence(), которая полезна двумя операциями:\n",
        "\n",
        "деление общей последовательности на части с помощью функции split_sequence();\n",
        "\n",
        "формирование бинарных (содержащих только 0 и 1) последовательностей верных ответов в соответствии с каждым примером для тренировочной и для тестовой выборок (метод to_categorical()).\n",
        "\n",
        "Тогда в результате использования vectorize_sequence() вы всегда получите два готовых numpy-массива. Если подадите в функцию данные для обучения, то это будет обучающая выборка и выборка ответов для нее. То же будет и в случае с формированием массивов на основе тестовых данных."
      ],
      "metadata": {
        "id": "vThVb08yCZoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция разбиения последовательности на отрезки скользящим окном\n",
        "# На входе - последовательность индексов, размер окна, шаг окна\n",
        "def split_sequence(sequence, win_size, hop):\n",
        "    # Последовательность разбивается на части до последнего полного окна\n",
        "    return [sequence[i:i + win_size] for i in range(0, len(sequence) - win_size + 1, hop)]\n",
        "\n",
        "\n",
        "# Функция формирования выборок из последовательностей индексов\n",
        "# формирует выборку отрезков и соответствующих им меток классов в виде one hot encoding\n",
        "def vectorize_sequence(seq_list, win_size, hop):\n",
        "    # В списке последовательности следуют в порядке их классов\n",
        "    # Всего последовательностей в списке ровно столько, сколько классов\n",
        "    class_count = len(seq_list)\n",
        "\n",
        "    # Списки для исходных векторов и категориальных меток класса\n",
        "    x, y = [], []\n",
        "\n",
        "    # Для каждого класса:\n",
        "    for cls in range(class_count):\n",
        "        # Разбиение последовательности класса cls на отрезки\n",
        "        vectors = split_sequence(seq_list[cls], win_size, hop)\n",
        "        # Добавление отрезков в выборку\n",
        "        x += vectors\n",
        "        # Для всех отрезков класса cls добавление меток класса в виде OHE\n",
        "        y += [utils.to_categorical(cls, class_count)] * len(vectors)\n",
        "\n",
        "    # Возврат результатов как numpy-массивов\n",
        "    return np.array(x), np.array(y)"
      ],
      "metadata": {
        "id": "ZE49JHhyCbE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сформируйте выборки, посмотрите на их размер и заодно измерьте время выполнения операции. Последнее может отличаться в зависимости от среды выполнения и мощностей ПК:"
      ],
      "metadata": {
        "id": "e915t3BxCkOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Формирование обучающей и тестовой выборок\n",
        "with timex():\n",
        "    # Формирование обучающей выборки\n",
        "    x_train, y_train = vectorize_sequence(seq_train, WIN_SIZE, WIN_HOP)\n",
        "    # Формирование тестовой выборки\n",
        "    x_test, y_test = vectorize_sequence(seq_test, WIN_SIZE, WIN_HOP)\n",
        "\n",
        "    # Проверка формы сформированных данных\n",
        "    print(x_train.shape, y_train.shape)\n",
        "    print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOPOjrP3ClUc",
        "outputId": "cc173b6e-c610-4b0b-cbaa-398e6e99fc8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0,) (0,)\n",
            "(0,) (0,)\n",
            "Время обработки: 0.00 с\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Один из примеров последовательности индексов для обучения сети выглядит так:"
      ],
      "metadata": {
        "id": "sT-wsA3rCtJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Вывод отрезка индексов тренировочной выборки\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "t0hv9zHdCuLs",
        "outputId": "2d227d35-2d4a-4189-c59d-c4f29ed04db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 0 is out of bounds for axis 0 with size 0",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-13005ba367fe>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Вывод отрезка индексов тренировочной выборки\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сформируйте отдельные обучающую и тестовую выборки в формате Bag Of Words, чтобы в дальнейшем использовать оба способа обработки текстов при обучении НС. И узнайте время выполнения этих двух операций:"
      ],
      "metadata": {
        "id": "Ck7hHPenC2jc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Формирование выборок текстов в виде разреженных матриц (bag of words)\n",
        "with timex():\n",
        "    # На входе .sequences_to_matrix() ожидает список, .tolist() выполняет преобразование типа\n",
        "    x_train_01 = tokenizer.sequences_to_matrix(x_train.tolist())\n",
        "    x_test_01 = tokenizer.sequences_to_matrix(x_test.tolist())\n",
        "\n",
        "    # Вывод формы обучающей выборки в виде разреженной матрицы Bag of Words\n",
        "    print(x_train_01.shape)\n",
        "    # Вывод фрагмента отрезка обучающего текста в виде Bag of Words\n",
        "    print(x_train_01[0][0:100])"
      ],
      "metadata": {
        "id": "eyU9EZdGC3zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Различные архитектуры нейронных сетей для классификации текста Сервисные функции Пусть код ниже не вводит вас в ступор, там просто добавлена пара созданных функций для удобства компиляции и обучения модели. Именно это и будет делать функция под именем compile_train_model(). В ее параметры вы передадите все, что необходимо для компиляции и обучения:\n",
        "\n",
        "● саму модель НС;\n",
        "\n",
        "● выборки;\n",
        "\n",
        "● функцию оптимизации;\n",
        "\n",
        "● количество эпох обучения;\n",
        "\n",
        "● размер шага по выборке;\n",
        "\n",
        "● размер полотна графика.\n",
        "\n",
        "Да, функция даже нарисует для вас графики точности и ошибки сети! Не зря вы познакомились с библиотекой matplotlib (блок кода по отрисовке графиков последний в данной функции).\n",
        "\n",
        "Также в ячейке ниже написана функция eval_model(). Она оценивает точность работы сети при распознавании текстов, дает статистику по верно и неверно распознанным классам, и рисует график-матрицу ошибки предсказания сети! Это очень красивый график, похожий на яркую шахматную доску. Как его правильно читать, вы узнаете чуть позже.\n",
        "\n",
        "Последняя функция compile_train_eval_model() объединяет в себе функционал двух предыдущих. Ей вы и будете пользоваться, чтобы в будущем строить разные архитектуры, при этом в одну строку задавая компиляцию модели, обучение и оценку точности!"
      ],
      "metadata": {
        "id": "uEuSGToHC_sM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция компиляции и обучения модели нейронной сети\n",
        "def compile_train_model(model,\n",
        "                        x_train,\n",
        "                        y_train,\n",
        "                        x_val,\n",
        "                        y_val,\n",
        "                        optimizer='adam',\n",
        "                        epochs=50,\n",
        "                        batch_size=128,\n",
        "                        figsize=(20, 5)):\n",
        "\n",
        "    # Компиляция модели\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # Вывод сводки\n",
        "    model.summary()\n",
        "\n",
        "    # Обучение модели с заданными параметрами\n",
        "    history = model.fit(x_train,\n",
        "                        y_train,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        validation_data=(x_val, y_val))\n",
        "\n",
        "    # Вывод графиков точности и ошибки\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
        "    fig.suptitle('График процесса обучения модели')\n",
        "    ax1.plot(history.history['accuracy'],\n",
        "               label='Доля верных ответов на обучающем наборе')\n",
        "    ax1.plot(history.history['val_accuracy'],\n",
        "               label='Доля верных ответов на проверочном наборе')\n",
        "    ax1.xaxis.get_major_locator().set_params(integer=True)\n",
        "    ax1.set_xlabel('Эпоха обучения')\n",
        "    ax1.set_ylabel('Доля верных ответов')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(history.history['loss'],\n",
        "               label='Ошибка на обучающем наборе')\n",
        "    ax2.plot(history.history['val_loss'],\n",
        "               label='Ошибка на проверочном наборе')\n",
        "    ax2.xaxis.get_major_locator().set_params(integer=True)\n",
        "    ax2.set_xlabel('Эпоха обучения')\n",
        "    ax2.set_ylabel('Ошибка')\n",
        "    ax2.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Функция вывода результатов оценки модели на заданных данных\n",
        "def eval_model(model, x, y_true,\n",
        "               class_labels=[],\n",
        "               cm_round=3,\n",
        "               title='',\n",
        "               figsize=(10, 10)):\n",
        "    # Вычисление предсказания сети\n",
        "    y_pred = model.predict(x)\n",
        "    # Построение матрицы ошибок\n",
        "    cm = confusion_matrix(np.argmax(y_true, axis=1),\n",
        "                          np.argmax(y_pred, axis=1),\n",
        "                          normalize='true')\n",
        "    # Округление значений матрицы ошибок\n",
        "    cm = np.around(cm, cm_round)\n",
        "\n",
        "    # Отрисовка матрицы ошибок\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    ax.set_title(f'Нейросеть {title}: матрица ошибок нормализованная', fontsize=18)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "    disp.plot(ax=ax)\n",
        "    plt.gca().images[-1].colorbar.remove()  # Стирание ненужной цветовой шкалы\n",
        "    plt.xlabel('Предсказанные классы', fontsize=16)\n",
        "    plt.ylabel('Верные классы', fontsize=16)\n",
        "    fig.autofmt_xdate(rotation=45)          # Наклон меток горизонтальной оси при необходимости\n",
        "    plt.show()\n",
        "\n",
        "    print('-'*100)\n",
        "    print(f'Нейросеть: {title}')\n",
        "\n",
        "    # Для каждого класса:\n",
        "    for cls in range(len(class_labels)):\n",
        "        # Определяется индекс класса с максимальным значением предсказания (уверенности)\n",
        "        cls_pred = np.argmax(cm[cls])\n",
        "        # Формируется сообщение о верности или неверности предсказания\n",
        "        msg = 'ВЕРНО :-)' if cls_pred == cls else 'НЕВЕРНО :-('\n",
        "        # Выводится текстовая информация о предсказанном классе и значении уверенности\n",
        "        print('Класс: {:<20} {:3.0f}% сеть отнесла к классу {:<20} - {}'.format(class_labels[cls],\n",
        "                                                                               100. * cm[cls, cls_pred],\n",
        "                                                                               class_labels[cls_pred],\n",
        "                                                                               msg))\n",
        "\n",
        "    # Средняя точность распознавания определяется как среднее диагональных элементов матрицы ошибок\n",
        "    print('\\nСредняя точность распознавания: {:3.0f}%'.format(100. * cm.diagonal().mean()))\n",
        "\n",
        "\n",
        "# Совместная функция обучения и оценки модели нейронной сети\n",
        "def compile_train_eval_model(model,\n",
        "                             x_train,\n",
        "                             y_train,\n",
        "                             x_test,\n",
        "                             y_test,\n",
        "                             class_labels=CLASS_LIST,\n",
        "                             title='',\n",
        "                             optimizer='adam',\n",
        "                             epochs=50,\n",
        "                             batch_size=128,\n",
        "                             graph_size=(20, 5),\n",
        "                             cm_size=(10, 10)):\n",
        "    # Компиляция и обучение модели на заданных параметрах\n",
        "    # В качестве проверочных используются тестовые данные\n",
        "    compile_train_model(model,\n",
        "                        x_train, y_train,\n",
        "                        x_test, y_test,\n",
        "                        optimizer=optimizer,\n",
        "                        epochs=epochs,\n",
        "                        batch_size=batch_size,\n",
        "                        figsize=graph_size)\n",
        "\n",
        "    # Вывод результатов оценки работы модели на тестовых данных\n",
        "    eval_model(model, x_test, y_test,\n",
        "               class_labels=class_labels,\n",
        "               title=title,\n",
        "               figsize=cm_size)"
      ],
      "metadata": {
        "id": "VyDErXaDDBS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуйте способ обучения сети на Bag Of Words. Сеть будет угадывать одного из шести авторов для каждого примера текста. На выходном слое нужно указать функцию активации softmax.\n",
        "\n",
        "Соберите полносвязную модель, подобную той, что вы уже рассматривали на первом занятии по НС. А на обучение и тест сети подайте выборки формата Bag Of Words (x_train_01 и x_test_01), которые вы подготовили ранее.\n",
        "\n",
        "Функция compile_train_eval_model() также выведет статистику по сформированной модели до начала обучения. Смотрите:"
      ],
      "metadata": {
        "id": "XXTmcPD1DQYs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание последовательной модели нейросети\n",
        "model_text_bow_softmax = Sequential()\n",
        "# Первый полносвязный слой\n",
        "model_text_bow_softmax.add(Dense(200, input_dim=VOCAB_SIZE, activation=\"relu\"))\n",
        "# Слой регуляризации Dropout\n",
        "model_text_bow_softmax.add(Dropout(0.25))\n",
        "# Слой пакетной нормализации\n",
        "model_text_bow_softmax.add(BatchNormalization())\n",
        "#Добавим второй полносвязный слой\n",
        "model_text_bow_softmax.add(Dense(100, input_dim=VOCAB_SIZE, activation=\"relu\"))\n",
        "# Еще один слой регуляризации Dropout\n",
        "model_text_bow_softmax.add(Dropout(0.15))\n",
        "#Добавляем еще один денс-слой\n",
        "model_text_bow_softmax.add(Dense(50, input_dim=VOCAB_SIZE, activation=\"relu\"))\n",
        "\n",
        "# Выходной полносвязный слой\n",
        "model_text_bow_softmax.add(Dense(CLASS_COUNT, activation='softmax'))\n",
        "\n",
        "# Входные данные подаются в виде векторов bag of words\n",
        "compile_train_eval_model(model_text_bow_softmax,\n",
        "                         x_train_01, y_train,\n",
        "                         x_test_01, y_test,\n",
        "                         class_labels=CLASS_LIST,\n",
        "                         title='BoW')"
      ],
      "metadata": {
        "id": "y0rgSH2hDRsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сохранение полной модели (архитектуры слоев и весов каждого слоя)\n",
        "# Для чтения модели используется метод keras.models.load_model()\n",
        "model_text_bow_softmax.save('model_text_bow_softmax.h5')"
      ],
      "metadata": {
        "id": "jM7GT8SuEoTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding. Как вы помните, этот слой создает из каждого индекса слова отдельный вектор числовых значений, размер которого равен параметру, указываемому при создании слоя в архитектуре.\n",
        "\n",
        "Первая модель Embedding создаст для каждого слова в выборке вектор из 20 значений. Размер вектора, который создает слой, указывается в параметре output_dim. Это довольно малый размер, сеть обучится быстро, но и точность может оказаться невысокой. Параметр input_dim указывает количество уникальных значений (токенов), который данных слой будет способен обработать (если мы установим значение input_dim = N и подадим токен больше чем N, этот слой выдаст ошибку, по скольку у него не будут нужные веса для обработки этого токена).\n",
        "\n",
        "Для работы с Embedding вы подадите выборки, созданные до преобразования в Bag Of Words – просто x_train и x_test:\n",
        "\n",
        "Embedding + Dense: размерность эмбеддингов 20"
      ],
      "metadata": {
        "id": "rEamCPcqE5Pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Архитектура со слоем Embedding и регуляризацией\n",
        "model_text_emb_20 = Sequential()\n",
        "model_text_emb_20.add(Embedding(input_dim = VOCAB_SIZE, output_dim = 20, input_length=WIN_SIZE))\n",
        "model_text_emb_20.add(SpatialDropout1D(0.2))\n",
        "model_text_emb_20.add(Flatten())\n",
        "model_text_emb_20.add(BatchNormalization())\n",
        "model_text_emb_20.add(Dense(200, activation=\"relu\"))\n",
        "model_text_emb_20.add(Dropout(0.2))\n",
        "model_text_emb_20.add(BatchNormalization())\n",
        "model_text_emb_20.add(Dense(CLASS_COUNT, activation='softmax'))\n",
        "\n",
        "# Входные данные подаются в виде последовательностей индексов,\n",
        "# а не векторов bag of words\n",
        "compile_train_eval_model(model_text_emb_20,\n",
        "                         x_train, y_train,\n",
        "                         x_test, y_test,\n",
        "                         class_labels=CLASS_LIST,\n",
        "                         title='Embedding/20')"
      ],
      "metadata": {
        "id": "eHQ1FByJE6nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выводы по итогам обучения модели Embedding(20) + Dense\n",
        "\n",
        "Посмотрите на графики и данные по эпохам. Точность и ошибка на проверочном наборе нарисовали пилу, хотя на обучающем по-прежнему полный штиль. Однако график матрицы ошибок не даст вам себя обмануть: вы видите, как изменилась тональность на диагонали и как упали значения точности. И правда, на последней эпохе сеть дала точность распознавания 70%. Результат модели Embedding пока что сильно уступает Bag Of Words.\n",
        "\n",
        "Посмотрите, как изменится точность, если увеличить размер Embedding-пространства до 200, а также добавить новый слой одномерной регуляризации SpatialDropout1D."
      ],
      "metadata": {
        "id": "OwhbeoEKFG_U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding + Dense: размерность эмбеддингов 200"
      ],
      "metadata": {
        "id": "imu6kFl5FMB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Создаём сеть с Embedding слоем\n",
        "model_text_emb_200 = Sequential()\n",
        "model_text_emb_200.add(Embedding(VOCAB_SIZE, 200, input_length=WIN_SIZE))\n",
        "model_text_emb_200.add(SpatialDropout1D(0.2))\n",
        "model_text_emb_200.add(Flatten())\n",
        "model_text_emb_200.add(BatchNormalization())\n",
        "model_text_emb_200.add(Dense(200, activation=\"relu\"))\n",
        "model_text_emb_200.add(Dropout(0.2))\n",
        "model_text_emb_200.add(BatchNormalization())\n",
        "model_text_emb_200.add(Dense(CLASS_COUNT, activation='softmax'))\n",
        "\n",
        "compile_train_eval_model(model_text_emb_200,\n",
        "                         x_train, y_train,\n",
        "                         x_test, y_test,\n",
        "                         class_labels=CLASS_LIST,\n",
        "                         title='Embedding/200')"
      ],
      "metadata": {
        "id": "rv2GaWQrFUUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wzVm1uGFIKV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}