{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitrijLeontev/Dmitrij_Leontev/blob/main/homework/DzPro16_%D0%B7%D0%B0%D0%BD%D1%8F%D1%82%D0%B8%D0%B5ChatGPTProfessional.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмите видео с ютуб, аудио-файл и текстовый документ по ссылкам, транскрибируйте видео и аудио -дорожку, полученные тексты соедините с текстовым документом, на основе получившегося текста создайте нейро-консультанта для ответов на вопросы по данным материалам, а также краткую методичку.\n",
        "\n",
        "Видео: https://www.youtube.com/watch?v=LRuszWRN4uI&list=PLv_mO3iQ2o2fZllHFsEniuy1D-2IXr8eU Python в вопросах и ответах\n",
        "\n",
        "Аудио: https://drive.google.com/file/d/1TLsB8EEMqATs18Iwacw5H3nceOFeO7ND/view?usp=sharing ООП в Пайтон\n",
        "\n",
        "Текст: https://docs.google.com/document/d/1GwrAY4NoqPImiSMola-14ykVB5K-8iDlivWzvuEnMfA/edit?usp=sharing Объяснение алгоритмов сортировки с примерами на Python\n",
        "\n",
        "P.S. аудио-файлы для обработки whisper стоит сохранить с именем на латинице и без пробелов в названии."
      ],
      "metadata": {
        "id": "CT0jE0Wip3OS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lnfPNAhDJk6"
      },
      "outputs": [],
      "source": [
        "yt_urls = ['https://www.youtube.com/watch?v=LRuszWRN4uI&list=PLv_mO3iQ2o2fZllHFsEniuy1D-2IXr8eU']                  # Впишите сюда ссылку на видео\n",
        "YouTube_video_title = \"Python в вопросах и ответах\"                                                                # Впишите сюда название видео"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/ytdl-org/youtube-dl.git\n",
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "IpuJ1EFMsXrA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee4b4e1c-3277-45c5-e6f3-9735f2502468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/ytdl-org/youtube-dl.git\n",
            "  Cloning https://github.com/ytdl-org/youtube-dl.git to /tmp/pip-req-build-kej30dnj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ytdl-org/youtube-dl.git /tmp/pip-req-build-kej30dnj\n",
            "  Resolved https://github.com/ytdl-org/youtube-dl.git to commit be008e657d79832642e2158557c899249c9e31cd\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-eo_0ps5a\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-eo_0ps5a\n",
            "  Resolved https://github.com/openai/whisper.git to commit 8bc8860694949db53c42ba47ddc23786c2e02a8b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (1.23.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (4.66.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (10.1.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (0.5.2)\n",
            "Requirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117) (2.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117) (3.13.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117) (0.41.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20231117) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117) (2023.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20231117) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20231117) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken==0.4.0 openai==0.28.0 langchain==0.0.281 faiss-cpu==1.7.4"
      ],
      "metadata": {
        "id": "YhSNcXPGXaVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2357ce2-f5fa-484b-a0cb-766ec5b76bba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken==0.4.0 in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: openai==0.28.0 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: langchain==0.0.281 in /usr/local/lib/python3.10/dist-packages (0.0.281)\n",
            "Requirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.4.0) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.0) (3.9.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (2.0.23)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.21 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (0.0.70)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (2.8.8)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (1.10.13)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.281) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.0) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.281) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.281) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.281) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken==0.4.0) (2023.11.17)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.281) (3.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.281) (23.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.281) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq python-docx\n",
        "!pip install -U pytube\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "gl3qGHwBsXtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from IPython.display import HTML, clear_output\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "import json\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import getpass\n",
        "import pickle\n",
        "#import torch\n",
        "from pytube import YouTube\n",
        "import tiktoken\n",
        "from docx import Document\n",
        "from docx.enum.text import WD_PARAGRAPH_ALIGNMENT\n",
        "import whisper\n",
        "import openai\n",
        "\n",
        "# Сохраняем во временное хранилище\n",
        "import codecs\n",
        "from langchain.chains import ConversationChain         # Импортируем класс для создания цепочек диалогов\n",
        "from langchain.chat_models import ChatOpenAI           # Импортируем класс для работы с чатами на базе OpenAI\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.memory import ConversationBufferMemory  # Импортируем класс для управления памятью диалогов\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter, Document, RecursiveCharacterTextSplitter\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "WzKd-6qesXwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Получаем первую (и единственную) ссылку из списка yt_urls\n",
        "url = yt_urls[0]\n",
        "\n",
        "# Задаем путь и формат названия файла\n",
        "output_template = f'tmp/{YouTube_video_title}.m4a'\n",
        "\n",
        "# Загружаем аудио с лучшим качеством (в формате m4a) и сохраняем его под определенным именем\n",
        "!youtube-dl $url -f 'bestaudio[ext=m4a]' -o \"$output_template\"\n",
        "\n",
        "# Указываем путь к файлу, который будем обрабатывать\n",
        "mp4_file = output_template"
      ],
      "metadata": {
        "id": "BbBjWW8zL6DY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "962bd76a-c5be-490d-c875-5d0892da8e37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: -f: command not found\n",
            "[youtube] LRuszWRN4uI: Downloading webpage\n",
            "[youtube] LRuszWRN4uI: Downloading player d23221b6\n",
            "\u001b[0;33mWARNING:\u001b[0m Requested formats are incompatible for merge and will be merged into mkv.\n",
            "[dashsegments] Total fragments: 8\n",
            "[download] Destination: Изучаем Python _ 1 глава - 'Python в вопросах и ответах' с Алексеем Трояновым-LRuszWRN4uI.f248.webm\n",
            "\u001b[K[download] 100% of 71.91MiB in 00:01\n",
            "[dashsegments] Total fragments: 4\n",
            "[download] Destination: Изучаем Python _ 1 глава - 'Python в вопросах и ответах' с Алексеем Трояновым-LRuszWRN4uI.f140.m4a\n",
            "\u001b[K[download] 100% of 33.42MiB in 00:02\n",
            "[ffmpeg] Merging formats into \"Изучаем Python _ 1 глава - 'Python в вопросах и ответах' с Алексеем Трояновым-LRuszWRN4uI.mkv\"\n",
            "Deleting original file Изучаем Python _ 1 глава - 'Python в вопросах и ответах' с Алексеем Трояновым-LRuszWRN4uI.f248.webm (pass -k to keep)\n",
            "Deleting original file Изучаем Python _ 1 глава - 'Python в вопросах и ответах' с Алексеем Трояновым-LRuszWRN4uI.f140.m4a (pass -k to keep)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Используем Whisper для обработки аудиофайла\n",
        "!whisper \"$output_template\" --model large --language Russian # добавьте сюда значение для перевода транскрибированного текста на английский язык - можно посмотреть в документации"
      ],
      "metadata": {
        "id": "76uZ-0s4sX1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2068c223-3b07-45a6-a050-2f6ccdd6a26d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|██████████████████████████████████████| 2.88G/2.88G [00:25<00:00, 122MiB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py:115: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 58, in load_audio\n",
            "    out = run(cmd, capture_output=True, check=True).stdout\n",
            "  File \"/usr/lib/python3.10/subprocess.py\", line 526, in run\n",
            "    raise CalledProcessError(retcode, process.args,\n",
            "subprocess.CalledProcessError: Command '['ffmpeg', '-nostdin', '-threads', '0', '-i', 'tmp/Python в вопросах и ответах.m4a', '-f', 's16le', '-ac', '1', '-acodec', 'pcm_s16le', '-ar', '16000', '-']' returned non-zero exit status 1.\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 478, in cli\n",
            "    result = transcribe(model, audio_path, temperature=temperature, **args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/transcribe.py\", line 122, in transcribe\n",
            "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 140, in log_mel_spectrogram\n",
            "    audio = load_audio(audio)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/whisper/audio.py\", line 60, in load_audio\n",
            "    raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
            "RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "tmp/Python в вопросах и ответах.m4a: No such file or directory\n",
            "\n",
            "Skipping tmp/Python в вопросах и ответах.m4a due to RuntimeError: Failed to load audio: ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "  libavutil      56. 70.100 / 56. 70.100\n",
            "  libavcodec     58.134.100 / 58.134.100\n",
            "  libavformat    58. 76.100 / 58. 76.100\n",
            "  libavdevice    58. 13.100 / 58. 13.100\n",
            "  libavfilter     7.110.100 /  7.110.100\n",
            "  libswscale      5.  9.100 /  5.  9.100\n",
            "  libswresample   3.  9.100 /  3.  9.100\n",
            "  libpostproc    55.  9.100 / 55.  9.100\n",
            "tmp/Python в вопросах и ответах.m4a: No such file or directory\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_key_ОpenAI():\n",
        "  openai.api_key = getpass.getpass(prompt='Введите секретный ключ для сервиса chatGPT: ')\n",
        "  os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n",
        "\n",
        "get_key_ОpenAI()"
      ],
      "metadata": {
        "id": "31YYrVlfTDp9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "491210bb-1fac-428a-91af-1d9918541a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите секретный ключ для сервиса chatGPT: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Напишите самостоятельно промпт в систем и юзер для разделения текста на логические блоки с выделением названия раздела:\n",
        "system = '\\u0412\\u044B \\u0433\\u0435\\u043D\\u0438\\u0439 \\u0442\\u0435\\u043A\\u0441\\u0442\\u0430, \\u043A\\u043E\\u043F\\u0438\\u0440\\u0430\\u0439\\u0442\\u0438\\u043D\\u0433\\u0430, \\u043F\\u0438\\u0441\\u0430\\u0442\\u0435\\u043B\\u044C\\u0441\\u0442\\u0432\\u0430. \\u0412\\u0430\\u0448\\u0430 \\u0437\\u0430\\u0434\\u0430\\u0447\\u0430 \\u0440\\u0430\\u0441\\u043F\\u043E\\u0437\\u043D\\u0430\\u0442\\u044C \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u044B \\u0432 \\u0442\\u0435\\u043A\\u0441\\u0442\\u0435 \\u0438 \\u0440\\u0430\\u0437\\u0431\\u0438\\u0442\\u044C \\u0435\\u0433\\u043E \\u043D\\u0430 \\u044D\\u0442\\u0438 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u044B \\u0441\\u043E\\u0445\\u0440\\u0430\\u043D\\u044F\\u044F \\u0432\\u0435\\u0441\\u044C \\u0442\\u0435\\u043A\\u0441\\u0442 \\u043D\\u0430 100%. ' #@param {type:\"string\"}\n",
        "user = \"\\u041F\\u043E\\u0436\\u0430\\u043B\\u0443\\u0439\\u0441\\u0442\\u0430, \\u0434\\u0430\\u0432\\u0430\\u0439\\u0442\\u0435 \\u043F\\u043E\\u0434\\u0443\\u043C\\u0430\\u0435\\u043C \\u0448\\u0430\\u0433 \\u0437\\u0430 \\u0448\\u0430\\u0433\\u043E\\u043C: \\u041F\\u043E\\u0434\\u0443\\u043C\\u0430\\u0439\\u0442\\u0435, \\u043A\\u0430\\u043A\\u0438\\u0435 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u044B \\u0432 \\u0442\\u0435\\u043A\\u0441\\u0442\\u0435 \\u0432\\u044B \\u043C\\u043E\\u0436\\u0435\\u0442\\u0435 \\u0440\\u0430\\u0441\\u043F\\u043E\\u0437\\u043D\\u0430\\u0442\\u044C \\u0438 \\u043A\\u0430\\u043A\\u043E\\u0435 \\u043D\\u0430\\u0437\\u0432\\u0430\\u043D\\u0438\\u0435 \\u043F\\u043E \\u0441\\u043C\\u044B\\u0441\\u043B\\u0443 \\u043C\\u043E\\u0436\\u043D\\u043E \\u0434\\u0430\\u0442\\u044C \\u043A\\u0430\\u0436\\u0434\\u043E\\u043C\\u0443 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u0443. \\u0414\\u0430\\u043B\\u0435\\u0435 \\u043D\\u0430\\u043F\\u0438\\u0448\\u0438\\u0442\\u0435 \\u043E\\u0442\\u0432\\u0435\\u0442 \\u043F\\u043E \\u0432\\u0441\\u0435\\u043C\\u0443 \\u043F\\u0440\\u0435\\u0434\\u044B\\u0434\\u0443\\u0449\\u0435\\u043C\\u0443 \\u043E\\u0442\\u0432\\u0435\\u0442\\u0443 \\u0432 \\u043F\\u043E\\u0440\\u044F\\u0434\\u043A\\u0435: ## \\u041D\\u0430\\u0437\\u0432\\u0430\\u043D\\u0438\\u0435 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u0430, \\u043F\\u043E\\u0441\\u043B\\u0435 \\u0447\\u0435\\u0433\\u043E \\u0432\\u0435\\u0441\\u044C \\u0442\\u0435\\u043A\\u0441\\u0442, \\u043E\\u0442\\u043D\\u043E\\u0441\\u044F\\u0449\\u0438\\u0439\\u0441\\u044F \\u043A \\u044D\\u0442\\u043E\\u043C\\u0443 \\u0440\\u0430\\u0437\\u0434\\u0435\\u043B\\u0443. \\u0422\\u0435\\u043A\\u0441\\u0442:\" #@param {type:\"string\"}\n",
        "\n",
        "temperature = 0 #@param {type: \"slider\", min: 0, max: 1, step:0.1}\n",
        "chunk_size = 5000 #@param {type: \"slider\", min: 1000, max: 7000, step:500}"
      ],
      "metadata": {
        "id": "U8okE5h5TDsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Функции\n",
        "# Функция настройки стиля для переноса текста в выводе ячеек\n",
        "# для изменения стиля отображения текста, так чтобы предотвратить переполнение текста за границы ячейки вывода и обеспечить его перенос.\n",
        "def set_text_wrap_css():\n",
        "    css = '''\n",
        "    <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "    </style>\n",
        "    '''\n",
        "    display(HTML(css))\n",
        "\n",
        "get_ipython().events.register('pre_run_cell', set_text_wrap_css)\n",
        "\n",
        "# Функция подсчета количества токенов\n",
        "def num_tokens_from_messages(messages, model='gpt-3.5-turbo-0301'):\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding('cl100k_base')\n",
        "\n",
        "    if model in ['gpt-3.5-turbo-0301', 'gpt-3.5-turbo-0613', 'gpt-3.5-turbo-16k', 'gpt-3.5-turbo', 'gpt-3.5-turbo-1106']:\n",
        "        num_tokens = 0\n",
        "\n",
        "        for message in messages:\n",
        "            num_tokens += 4\n",
        "\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "\n",
        "                if key == 'name':\n",
        "                    num_tokens -= 1\n",
        "\n",
        "        num_tokens += 2\n",
        "        return num_tokens\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(f'''num_tokens_from_messages() is not presently implemented for model {model}.''')\n",
        "\n",
        "\n",
        "# Функция дробления текста на чанки\n",
        "def split_text(txt_file, chunk_size=chunk_size):\n",
        "    source_chunks = []\n",
        "    splitter = RecursiveCharacterTextSplitter(separators=['\\n', '\\n\\n', '. '], chunk_size=chunk_size, chunk_overlap=0)\n",
        "\n",
        "    for chunk in splitter.split_text(txt_file):\n",
        "        source_chunks.append(Document(page_content=chunk, metadata={}))\n",
        "\n",
        "    print(f'\\n\\nТекст разбит на {len(source_chunks)} чанков.')\n",
        "\n",
        "    return source_chunks\n",
        "\n",
        "\n",
        "# Функция получения ответа от модели\n",
        "def answer_index(system, user, chunk, temp=temperature, model='gpt-3.5-turbo'):\n",
        "\n",
        "    messages = [\n",
        "        {'role': 'system', 'content': system},\n",
        "        {'role': 'user', 'content': user + f'{chunk}'}\n",
        "    ]\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=messages,\n",
        "        temperature=temp\n",
        "    )\n",
        "\n",
        "    # Вывод количества токенов отключен\n",
        "    # print(f'\\n====================\\n\\n{num_tokens_from_messages(messages)} токенов будет использовано на чанк\\n\\n')\n",
        "    answer = completion.choices[0].message.content\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "def process_one_file(file_path, system, user):\n",
        "    with open(file_path, 'r') as txt_file:\n",
        "        text = txt_file.read()\n",
        "    source_chunks = split_text(text)\n",
        "    processed_text = ''\n",
        "    unprocessed_text = ''\n",
        "\n",
        "    for chunk in source_chunks:\n",
        "        attempt = 0\n",
        "\n",
        "        while attempt < 3:\n",
        "            try:\n",
        "                answer = answer_index(system, user, chunk.page_content)\n",
        "                break  # Успешно получили ответ, выходим из цикла попыток\n",
        "\n",
        "            except Exception as e:\n",
        "                attempt += 1  # Увеличиваем счетчик попыток\n",
        "                print(f'\\n\\nПопытка {attempt} не удалась из-за ошибки: {str(e)}')\n",
        "                time.sleep(10)  # Ожидаем перед следующей попыткой\n",
        "                if attempt == 3:\n",
        "                    answer = ''\n",
        "                    print(f'\\n\\nОбработка элемента {chunk} не удалась после 3 попыток')\n",
        "                    unprocessed_text += f'{chunk}\\n\\n'\n",
        "\n",
        "        processed_text += f'{answer}\\n\\n'  # Добавляем ответ в результат\n",
        "        print(f'{answer}')  # Выводим ответ\n",
        "\n",
        "    return processed_text, unprocessed_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "xtJbCJFYNn44",
        "outputId": "b8499fb6-36db-4c0b-eda0-60d25758dd6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Запуск\n",
        "file_path = f'/content/{YouTube_video_title}.txt'\n",
        "# Вызываем функцию обработки для этого файла\n",
        "processed_text, unprocessed_text = process_one_file(file_path, system, user)"
      ],
      "metadata": {
        "id": "aT81HHTiN4V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# теперь в переменной\n",
        "processed_text"
      ],
      "metadata": {
        "id": "ET2aiCCSd1Hj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итак, мы получили текст транскрибации, разделенный на разделы с названиями данных разделов. Теперь разделим этот текст на чанки при помощи MarkdownHeaderTextSplitter, создадим из него векторную базу Faiss и сделаем нейро-консультанта, который отвечает на вопросы по тексту на русском языке, а из текста транскрибации с разделами составим методичку на английском."
      ],
      "metadata": {
        "id": "vdjA4F4Qz-n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Создаем нейро-консультанта:"
      ],
      "metadata": {
        "id": "QayojKuq2bTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определяем заголовки, на которые следует разбить текст\n",
        "headers_to_split_on = [\n",
        "    (\"##\", \"Header 2\")\n",
        "    ]\n",
        "# Создаем объект для разбиения текста на секции по заголовкам\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# Получаем список документов, разбитых по заголовкам\n",
        "md_header_splits = markdown_splitter.split_text(processed_text)"
      ],
      "metadata": {
        "id": "AlvFb0UJd1Km",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "0167f6e1-b4da-4d35-ecf8-3bb9cc6c62a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "md_header_splits"
      ],
      "metadata": {
        "id": "eV3ioiUAd1M0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Инициализирум модель эмбеддингов\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# Создадим индексную базу из разделенных фрагментов текста\n",
        "db = FAISS.from_documents(md_header_splits, embeddings)"
      ],
      "metadata": {
        "id": "Om4SLIVZ1Sxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Самостоятельно напишите промпт в систем для нейро-ассистента.\n",
        "system_for_NA = \"\""
      ],
      "metadata": {
        "id": "w_UI9Kcj1S36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_neuro_assist(system, topic, search_index, verbose=1):\n",
        "\n",
        "    # Поиск релевантных отрезков из базы знаний\n",
        "    docs = search_index.similarity_search(topic, k=3)\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "    message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\nExcerpt of document №{i+1}\\n=====================' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "    if verbose: print('message_content :\\n ======================================== \\n', message_content)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_for_NA},\n",
        "        {\"role\": \"user\", \"content\": f\"Answer the student's question. Answer in Russian. Do not mention the excerpts of the student response information document in your answer. Student response document: {message_content}\\n\\nStudent Question: \\n{topic}\"}\n",
        "    ]\n",
        "\n",
        "    if verbose: print('\\n ===========================================: ')\n",
        "\n",
        "    completion = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=messages,\n",
        "        temperature=0\n",
        "    )\n",
        "    answer = completion.choices[0].message.content\n",
        "    return answer  # возвращает ответ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "puHiSfgs1S64",
        "outputId": "03a52156-bbdb-41c3-f11e-5651607fd393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Проверьте работу нейро-ассистента на самостоятельно сформулированных вопросах\n",
        "topic=\"\"\n",
        "ans=answer_neuro_assist(system, topic, db, verbose=1)\n",
        "ans"
      ],
      "metadata": {
        "id": "wTJ9h1L91S97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic=\"\"\n",
        "ans=answer_neuro_assist(system, topic, db, verbose=1)\n",
        "ans"
      ],
      "metadata": {
        "id": "A_VM1VEP1TAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Делаем методичку:"
      ],
      "metadata": {
        "id": "CiT2_nn46h-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Обрабатываем каждый чанк, выделяя только суть для методички\n",
        "system = \"You are a copywriting genius, an expert in Python development interview preparation.. You receive a section of raw text on a certain topic. You need to extract the essence from this text, only the most important, keeping all the necessary details, but removing all the \\\"water\\\" and words (sentences) that do not carry a semantic load.\" #@param {type:\"string\"}\n",
        "user = \"From this text, select only the information that is valuable in terms of the topic \\\"Python development interview preparation.\\\". Remove all the \\\"water\\\". As a result, you should have a section for a manual on this topic. Base it only on the text given to you, don't make up anything of your own. The answer should be in the format ## Title of the text, and then the valuable information from the text. If there is no valuable information in the text, then leave only the title of the section, for example: \\\"## Introduction\\\". Text:\" #@param {type:\"string\"}\n",
        "\n",
        "temperature = 0 #@param {type: \"slider\", min: 0, max: 1, step:0.1}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "cellView": "form",
        "id": "NEWScxy26iJA",
        "outputId": "8e647667-408d-4caa-9320-ea0738f9f406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "    </style>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_documents(documents, system, user, temperature):\n",
        "    \"\"\"\n",
        "    Функция принимает чанки, system, user, temperature для модели.\n",
        "    Она обрабатывает каждый документ, используя модель GPT, конкатенирует результаты в один текст и сохраняет в файл .txt.\n",
        "    \"\"\"\n",
        "    processed_text_for_handbook = \"\"  # Строка для конкатенации обработанного текста\n",
        "\n",
        "    for document in documents:\n",
        "        # Форматируем метаданные для включения в чанк\n",
        "        metadata_str = \"\\n\".join([f\"{key}: {value}\" for key, value in document.metadata.items()])\n",
        "        # Конкатенируем метаданные и содержание документа для передачи в функцию\n",
        "        chunk_with_metadata = f\"{metadata_str}\\n\\n{document.page_content}\"\n",
        "\n",
        "        # Получаем ответ от модели\n",
        "        answer = answer_index(system, user, chunk_with_metadata, temperature, model='gpt-3.5-turbo')\n",
        "        # Добавляем обработанный текст в общую строку\n",
        "        processed_text_for_handbook += f\"{answer}\\n\\n\"\n",
        "\n",
        "    # Записываем полученный текст в файл\n",
        "    with open('processed_documents.txt', 'w', encoding='utf-8') as f:\n",
        "        f.write(processed_text_for_handbook)\n",
        "\n",
        "    # Функция возвращает путь к файлу с обработанным текстом\n",
        "    return 'processed_documents.txt'\n",
        "\n",
        "# Применение функции\n",
        "file_path = process_documents(md_header_splits, system, user, temperature)\n",
        "print(f\"Обработанный текст сохранен в файле: {file_path}\")"
      ],
      "metadata": {
        "id": "2jMwlo4R6iMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Прочитайте и выведите содержимое методички:\n"
      ],
      "metadata": {
        "id": "omh2GDts6iOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}