{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DmitrijLeontev/Dmitrij_Leontev/blob/main/homework/24(21)_light_%D0%97%D0%B0%D0%BF%D1%83%D1%81%D0%BA_%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5_SoTA(state_of_the_art)_%D0%BB%D0%BE%D0%BA%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_(%D0%BA%D0%BE%D0%BD%D1%82%D1%83%D1%80%D0%BD%D1%8B%D1%85)_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9_%D0%B8%D0%B7_%D0%BE%D1%82%D0%BA%D1%80%D1%8B%D1%82%D0%BE%D0%B9_%D1%82%D0%B0%D0%B1%D0%BB%D0%B8%D1%86%D1%8B_%D0%BB%D0%B8%D0%B4%D0%B5%D1%80%D0%BE%D0%B2_LLM_HuggingFace_%D0%B4%D0%BB%D1%8F_%D0%B3%D0%B5%D0%BD%D0%B5%D1%80%D0%B0%D1%86%D0%B8%D0%B8_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### –î–æ–º–∞—à–Ω–µ–µ –∑–∞–¥–∞–Ω–∏–µ: –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π LLM —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "**–¶–µ–ª—å –∑–∞–¥–∞–Ω–∏—è:** –ò—Å–ø–æ–ª—å–∑—É—è –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–Ω–µ—à–Ω–∏–µ —Ä–µ—Å—É—Ä—Å—ã –∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ä–∞–±–æ—Ç—ã —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π\n",
        "\n",
        "üìç –ú–æ–∂–Ω–æ –≤—ã–±—Ä–∞—Ç—å –ª–µ–≥–∫–æ–≤–µ—Å—Ç–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –Ω–∞ –ë–ï–°–ü–õ–ê–¢–ù–û–ú colab.google.\n",
        "\n",
        "\n",
        "\n",
        "**–ú–∞—Ç–µ—Ä–∏–∞–ª—ã –¥–ª—è –∑–∞–¥–∞–Ω–∏—è:**\n",
        "\n",
        "1. **–ë–∞–∑–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:** [–°—Å—ã–ª–∫–∞ –Ω–∞ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö](https://drive.google.com/file/d/1-gRMM-kX0WbCwFWzSr-6mvj-HZx9tGFf/view?usp=sharing). –≠—Ç–∞ –±–∞–∑–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä—É—é –≤—ã –±—É–¥–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª—è–º–∏.\n",
        "2. **–í–æ–ø—Ä–æ—Å—ã –¥–ª—è –º–æ–¥–µ–ª–µ–π:** [–°—Å—ã–ª–∫–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã](https://drive.google.com/file/d/1DyVljnB5K5cHX_Mod0P9CPxBE6syGH4n/view?usp=sharing). –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–∏ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π.\n",
        "3. **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –±–∞–∑–∞:** [–°—Å—ã–ª–∫–∞ –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –±–∞–∑—É](https://drive.google.com/file/d/1gY2LYmEsdLLDJyl0kWhrthCtEncKCOd-/view?usp=sharing). –ú–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –±–∞–∑—É –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞, –ø—Ä–∏–¥—É–º—ã–≤–∞—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –±–∞–∑—ã.\n",
        "\n",
        "**–ó–∞–¥–∞—á–∏:**\n",
        "\n",
        "1. **–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å –ø–æ–º–æ—â—å—é –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π. –û—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –∏—Å–ø–æ–ª—å–∑—É—è —à–∫–∞–ª—É –æ—Ç -2 –¥–æ 2, –≥–¥–µ -2 –æ–∑–Ω–∞—á–∞–µ—Ç –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç, –∞ +2 ‚Äî –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ç–æ—á–Ω—ã–π –æ—Ç–≤–µ—Ç.\n",
        "\n",
        "2. **–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π:** –ó–∞–ø–∏—à–∏—Ç–µ –≤—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –∫–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –∏ —Å—Ä–∞–≤–Ω–∏—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–µ–π –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∞–ø–ø–∞—Ä–∞—Ç–Ω—ã—Ö —Å—Ä–µ–¥—Å—Ç–≤–∞—Ö, –µ—Å–ª–∏ —ç—Ç–æ –≤–æ–∑–º–æ–∂–Ω–æ.\n",
        "\n",
        "3. **–¢–≤–æ—Ä—á–µ—Å–∫–∞—è –∑–∞–¥–∞—á–∞:** –ò—Å–ø–æ–ª—å–∑—É—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö, –ø—Ä–∏–¥—É–º–∞–π—Ç–µ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—é –±–∞–∑—ã. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –º–æ–¥–µ–ª–∏ –Ω–∞ —ç—Ç–∏—Ö –≤–æ–ø—Ä–æ—Å–∞—Ö –∏ –æ—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏—Ö –æ—Ç–≤–µ—Ç–æ–≤.\n",
        "\n",
        "**–û—Ç—á–µ—Ç:** –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –æ—Ç—á–µ—Ç –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –≤–∞—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞. –í –æ—Ç—á–µ—Ç–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–∫–∞–∑–∞–Ω—ã:\n",
        "- –í—ã–±—Ä–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏.\n",
        "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –ø—Ä–∏–¥—É–º–∞–Ω–Ω—ã–µ –≤–∞–º–∏ –≤–æ–ø—Ä–æ—Å—ã.\n",
        "- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–µ–π.\n",
        "- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞ –º–æ–¥–µ–ª–µ–π.\n",
        "- –í–∞—à–∏ –≤—ã–≤–æ–¥—ã –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ –∫–∞—á–µ—Å—Ç–≤–µ –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ –∏ –∞–ø–ø–∞—Ä–∞—Ç–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏.\n",
        "\n",
        "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:** –ü—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞–Ω–∏—è –æ–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π. –≠—Ç–æ –ø–æ–º–æ–∂–µ—Ç –≤–∞–º –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –º–æ–¥–µ–ª—è–º–∏ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ."
      ],
      "metadata": {
        "id": "q32zUov_NmTi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain –∏ llama-cpp-python\n",
        "\n",
        "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞:"
      ],
      "metadata": {
        "id": "-qq5Lj1JcFCR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü—Ä–∏–º–µ—Ä—ã —Ç–µ–º–ø–ª–µ–π—Ç–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π"
      ],
      "metadata": {
        "id": "1TKfyxi96kxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Platypus 2"
      ],
      "metadata": {
        "id": "D2r_gJeV7C7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{prompt}\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "g3hOrerj7D_R",
        "outputId": "07bcc304-053e-4b0d-856a-9dccccfaa6fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{prompt}\\n\\n### Response:\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama 2"
      ],
      "metadata": {
        "id": "tGCTMjRT6r_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        " [INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "\n",
        "{prompt} [/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "oAvRPylW6mER",
        "outputId": "ffe9d969-527e-4c7a-e148-91a1fee744b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n [INST] <<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n<</SYS>>\\n\\n{prompt} [/INST]\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "StableBeluga 2"
      ],
      "metadata": {
        "id": "hhaciyb4613Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "### System: You are a helpful, respectful and honest assistant.\n",
        "\n",
        "### User: {prompt}\n",
        "\n",
        "### Assistant:\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NoS3wzjl628A",
        "outputId": "9cfdee03-53f5-4e6d-9f88-ef7c4d524a5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n### System: You are a helpful, respectful and honest assistant.\\n\\n### User: {prompt}\\n\\n### Assistant:\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—É—Å–∫–∞ GGML, GGUF –º–æ–¥–µ–ª–µ–π"
      ],
      "metadata": {
        "id": "wCUZ6XCm7RHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) # –ö–æ–ª–±—ç–∫ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º\n",
        "llama = LlamaCpp(\n",
        "    model_path=\"/content/platypus2-70b-instruct.ggmlv3.q4_0.bin\", # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏\n",
        "    n_gpu_layers=70,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞ GPU. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1\n",
        "    n_batch=512, # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—Ç 1 –¥–æ n_ctx, —É—á–∏—Ç—ã–≤–∞—è –æ–±—ä–µ–º –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 512\n",
        "    n_gqa=8, # –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –º–æ–¥–µ–ª—è–º–∏ 70B\n",
        "    n_ctx=4096,  # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏\n",
        "    max_tokens=1000,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "    f16_kv=True,  # –î–û–õ–ñ–ù–û –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ True, –∏–Ω–∞—á–µ –ø–æ—Å–ª–µ –ø–∞—Ä—ã –≤—ã–∑–æ–≤–æ–≤ —É –≤–∞—Å –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –ø—Ä–æ–±–ª–µ–º—ã.\n",
        "    callback_manager=callback_manager, # –ö–æ–ª–±—ç–∫\n",
        "    verbose=True, # –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –Ω–µ—Ç\n",
        ")\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "1S2Yqw4S7KT3",
        "outputId": "32ec555a-8215-490c-9a4d-2fb465d98909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ncallback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) # –ö–æ–ª–±—ç–∫ –¥–ª—è —Å—Ç—Ä–∏–º–∏–Ω–≥–æ–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ —Ç–æ–∫–µ–Ω–∞–º\\nllama = LlamaCpp(\\n    model_path=\"/content/platypus2-70b-instruct.ggmlv3.q4_0.bin\", # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏\\n    n_gpu_layers=70,  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞ GPU. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 1\\n    n_batch=512, # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –æ—Ç 1 –¥–æ n_ctx, —É—á–∏—Ç—ã–≤–∞—è –æ–±—ä–µ–º –æ–ø–µ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–∞–º—è—Ç–∏. –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é 512\\n    n_gqa=8, # –ü–∞—Ä–∞–º–µ—Ç—Ä –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å –º–æ–¥–µ–ª—è–º–∏ 70B\\n    n_ctx=4096,  # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª–∏\\n    max_tokens=1000,  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\\n    f16_kv=True,  # –î–û–õ–ñ–ù–û –±—ã—Ç—å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ True, –∏–Ω–∞—á–µ –ø–æ—Å–ª–µ –ø–∞—Ä—ã –≤—ã–∑–æ–≤–æ–≤ —É –≤–∞—Å –≤–æ–∑–Ω–∏–∫–Ω—É—Ç –ø—Ä–æ–±–ª–µ–º—ã.\\n    callback_manager=callback_manager, # –ö–æ–ª–±—ç–∫\\n    verbose=True, # –û—Ç–æ–±—Ä–∞–∂–∞—Ç—å –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –≤—ã–≤–æ–¥ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –Ω–µ—Ç\\n)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "B9FQZqKtNobM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfb3da0e-07e1-45a4-ced6-d1d6bf6a671c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.76.tar.gz (49.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.11.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (3.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (2.1.5)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.76-cp310-cp310-linux_x86_64.whl size=3658152 sha256=b357a3d12d35d3aaebdcc33c5955032794735a692afc986ce150d4cf7e5a3f8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e5/04/a5fa9e60033548f205f0db5f6ab6f59cd27bd0da7f9c51cfe7\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞:"
      ],
      "metadata": {
        "id": "rxCQs8UAcinG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python --no-cache-dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeqhIOOUckJk",
        "outputId": "24c9b4c4-a535-4817-c6c0-fcc1d52fe89c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.76.tar.gz (49.4 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
            "  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m178.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python)\n",
            "  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m316.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
            "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.76-cp310-cp310-linux_x86_64.whl size=80963603 sha256=d6e9848cd91f51a5b0b5dd998d7d1078198480079eb047c351a20da6583e0ca8\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-62l4dwyu/wheels/a0/e5/04/a5fa9e60033548f205f0db5f6ab6f59cd27bd0da7f9c51cfe7\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.0\n",
            "    Uninstalling typing_extensions-4.12.0:\n",
            "      Successfully uninstalled typing_extensions-4.12.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 2.1.5\n",
            "    Uninstalling MarkupSafe-2.1.5:\n",
            "      Successfully uninstalled MarkupSafe-2.1.5\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.4\n",
            "    Uninstalling Jinja2-3.1.4:\n",
            "      Successfully uninstalled Jinja2-3.1.4\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.76\n",
            "    Uninstalling llama_cpp_python-0.2.76:\n",
            "      Successfully uninstalled llama_cpp_python-0.2.76\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.3.0+cu121 requires nvidia-cublas-cu12==12.1.3.1; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-cupti-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-nvrtc-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cuda-runtime-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cufft-cu12==11.0.2.54; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-curand-cu12==10.3.2.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cusolver-cu12==11.4.5.107; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-cusparse-cu12==12.1.0.106; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-nccl-cu12==2.20.5; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\n",
            "torch 2.3.0+cu121 requires nvidia-nvtx-cu12==12.1.105; platform_system == \"Linux\" and platform_machine == \"x86_64\", which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.4 llama-cpp-python-0.2.76 numpy-1.26.4 typing-extensions-4.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è ggml —Ñ–æ—Ä–º–∞—Ç–∞ –º–æ–¥–µ–ª–µ–π"
      ],
      "metadata": {
        "id": "WQAlPfZSc2E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.275 pydantic==1.10.13 openai==0.28.1 faiss-cpu==1.7.4 tiktoken==0.5.1 sentence_transformers==2.2.2 nltk==3.8.1\n",
        "!pip install -U deep-translator==1.11.4"
      ],
      "metadata": {
        "id": "z5b181euc3Nr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc04fce1-351e-4542-cec0-f0141bd42853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.275\n",
            "  Downloading langchain-0.0.275-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.13\n",
            "  Downloading pydantic-1.10.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu==1.7.4\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken==0.5.1\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence_transformers==2.2.2\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.275) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.275) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.275) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.275) (4.0.3)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain==0.0.275)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.21 (from langchain==0.0.275)\n",
            "  Downloading langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.275) (2.10.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.275) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.275) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.275) (8.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==1.10.13) (4.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.12.25)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (4.41.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.18.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.23.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.275) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.275) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.275) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.275) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.275) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.275)\n",
            "  Downloading marshmallow-3.21.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.275)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (24.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.275) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.275) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.275) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.275) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.275) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence_transformers==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.4.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers==2.2.2) (9.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain==0.0.275)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers==2.2.2) (1.3.0)\n",
            "Building wheels for collected packages: sentence_transformers\n",
            "  Building wheel for sentence_transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence_transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125924 sha256=06d0a1eea98bba3ed36127f1e938d48510556bbb431406749c2c352ad544a980\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "Successfully built sentence_transformers\n",
            "Installing collected packages: faiss-cpu, pydantic, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, typing-inspect, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, langsmith, openai, nvidia-cusolver-cu12, dataclasses-json, langchain, sentence_transformers\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.1\n",
            "    Uninstalling pydantic-2.7.1:\n",
            "      Successfully uninstalled pydantic-2.7.1\n",
            "Successfully installed dataclasses-json-0.5.14 faiss-cpu-1.7.4 langchain-0.0.275 langsmith-0.0.92 marshmallow-3.21.2 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 openai-0.28.1 pydantic-1.10.13 sentence_transformers-2.2.2 tiktoken-0.5.1 typing-inspect-0.9.0\n",
            "Collecting deep-translator==1.11.4\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m804.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator==1.11.4) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator==1.11.4) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator==1.11.4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator==1.11.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator==1.11.4) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator==1.11.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator==1.11.4) (2024.2.2)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING\"] = \"true\" # If you want to trace the execution of the program, set to \"true\""
      ],
      "metadata": {
        "id": "xDFQRk0-dCUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.1.77 --no-cache-dir"
      ],
      "metadata": {
        "id": "1pcCHxNidHYc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b7fb7da-9fe1-42f2-e3b9-f69f07343d1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.1.77\n",
            "  Downloading llama_cpp_python-0.1.77.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.1.77)\n",
            "  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.1.77)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.1.77)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m198.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.1.77-cp310-cp310-linux_x86_64.whl size=1365706 sha256=84d9383ed3f69b0ffac3ddec7a9806f2dd4e8490f166f2e03071077662ef0494\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z15kegt8/wheels/aa/ed/39/87f2ad350dbbf13b600ac744899186b8647c5323c62e2bb348\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.0\n",
            "    Uninstalling typing_extensions-4.12.0:\n",
            "      Successfully uninstalled typing_extensions-4.12.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.2.76\n",
            "    Uninstalling llama_cpp_python-0.2.76:\n",
            "      Successfully uninstalled llama_cpp_python-0.2.76\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.1.77 numpy-1.26.4 typing-extensions-4.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –¥–ª—è gguf —Ñ–æ—Ä–º–∞—Ç–∞ –º–æ–¥–µ–ª–µ–π"
      ],
      "metadata": {
        "id": "0r1tyy-fdMSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.322 pydantic openai==0.28.1 faiss-cpu==1.7.4 tiktoken==0.5.1 sentence_transformers==2.2.2 nltk==3.8.1\n",
        "!pip install -U deep-translator==1.11.4"
      ],
      "metadata": {
        "id": "nuT4FCxGdNij",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6389832-396d-42dd-9434-9c843e767665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.322\n",
            "  Downloading langchain-0.0.322-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (1.10.13)\n",
            "Requirement already satisfied: openai==0.28.1 in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: faiss-cpu==1.7.4 in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Requirement already satisfied: tiktoken==0.5.1 in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: sentence_transformers==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (3.9.5)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (0.5.14)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.322)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.43 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (0.0.92)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.322) (8.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken==0.5.1) (2023.12.25)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (4.41.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.18.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (1.11.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.1.99)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers==2.2.2) (0.23.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.4.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.322) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.322) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.322) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.322) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.322) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.322) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.322) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain==0.0.322) (1.2.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.322) (3.21.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.322) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers==2.2.2) (24.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.322)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.322) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.322) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.322) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.322) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->sentence_transformers==2.2.2) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence_transformers==2.2.2) (12.5.40)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers==2.2.2) (0.4.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers==2.2.2) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->sentence_transformers==2.2.2) (9.4.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.322) (1.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->sentence_transformers==2.2.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->sentence_transformers==2.2.2) (1.3.0)\n",
            "Installing collected packages: jsonpointer, jsonpatch, langchain\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.0.275\n",
            "    Uninstalling langchain-0.0.275:\n",
            "      Successfully uninstalled langchain-0.0.275\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.322\n",
            "Requirement already satisfied: deep-translator==1.11.4 in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator==1.11.4) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator==1.11.4) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator==1.11.4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator==1.11.4) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator==1.11.4) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator==1.11.4) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator==1.11.4) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING\"] = \"true\" # If you want to trace the execution of the program, set to \"true\""
      ],
      "metadata": {
        "id": "BwBl9QoxdWRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install --upgrade --force-reinstall llama-cpp-python==0.2.11 --no-cache-dir"
      ],
      "metadata": {
        "id": "RzpoSMEHdXnz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee45d99e-1114-4e7d-c6e3-b32d8eee29a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-cpp-python==0.2.11\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting typing-extensions>=4.5.0 (from llama-cpp-python==0.2.11)\n",
            "  Downloading typing_extensions-4.12.0-py3-none-any.whl (37 kB)\n",
            "Collecting numpy>=1.20.0 (from llama-cpp-python==0.2.11)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.11)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m229.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-linux_x86_64.whl size=6351292 sha256=022f2fbfabb6a7a05c5296e778b3842929dcba9783f4f050ea9573344a659b89\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pngq7hm6/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: typing-extensions, numpy, diskcache, llama-cpp-python\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.0\n",
            "    Uninstalling typing_extensions-4.12.0:\n",
            "      Successfully uninstalled typing_extensions-4.12.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: diskcache\n",
            "    Found existing installation: diskcache 5.6.3\n",
            "    Uninstalling diskcache-5.6.3:\n",
            "      Successfully uninstalled diskcache-5.6.3\n",
            "  Attempting uninstall: llama-cpp-python\n",
            "    Found existing installation: llama_cpp_python 0.1.77\n",
            "    Uninstalling llama_cpp_python-0.1.77:\n",
            "      Successfully uninstalled llama_cpp_python-0.1.77\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.11 numpy-1.26.4 typing-extensions-4.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–°–µ—Ä–≤–∏—Å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏"
      ],
      "metadata": {
        "id": "aSIYm5uXdf-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "id": "1MCX9S6edhPD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0edfe49e-5ea8-4334-eebf-b3223a528aa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lirZhXM1dnVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a4168e8-1269-4531-c249-82e1ab8d7f55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–Ω–∞ —Ä—É—Å—Å–∫–æ–º"
      ],
      "metadata": {
        "id": "dDPjUqRjdsEi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy\n",
        "!pip install numpy==1.24.4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "fmyI7SYJoiyT",
        "outputId": "af9da2d2-5bc9-40f6-d0c0-a50f948e23b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9b420b8e89b04a6d815f4abd107817cf"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/–ü—Ä–æ–µ–∫—Ç—ã/LLMs_local\"\n",
        "\n",
        "\n",
        "loader = TextLoader(\"/content/drive/MyDrive/Colab Notebooks/ChatGPTPro–î –∑/Dz_24(21)LLM_–õ–æ–∫–∞–ª—å–Ω—ã–µ_–∫–æ–Ω—Ç—É—Ä–Ω—ã–µ_–º–æ–¥–µ–ª–∏/–ë–∞–∑–∞_–∑–Ω–∞–Ω–∏–∏ÃÜ_–£–ò–ò_–í–µ—Ä—Å–∏—è_–æ—Ç_12_06_23.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)\n",
        "db = FAISS.from_documents(docs, OpenAIEmbeddings())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTT2XOdBlySI",
        "outputId": "a828734c-d8b8-487c-b17d-2639e88f3545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º"
      ],
      "metadata": {
        "id": "Z5GIbGJkdz_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "project_path = \"/content/drive/MyDrive\"\n",
        "\n",
        "\n",
        "loader = TextLoader(\"/content/drive/MyDrive/Colab Notebooks/ChatGPTPro–î –∑/Dz_24(21)LLM_–õ–æ–∫–∞–ª—å–Ω—ã–µ_–∫–æ–Ω—Ç—É—Ä–Ω—ã–µ_–º–æ–¥–µ–ª–∏/–ë–∞–∑–∞_–∑–Ω–∞–Ω–∏–∏ÃÜ_–£–ò–ò_–í–µ—Ä—Å–∏—è_–æ—Ç_12_06_23.txt\")\n",
        "documents = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=0)\n",
        "docs = text_splitter.split_documents(documents)"
      ],
      "metadata": {
        "id": "Gs-tgmdPdzk6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64c9a5b1-43c3-4eaa-b6c3-e4155a2e9154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "docs_translate = []\n",
        "for doc in tqdm.tqdm(docs):\n",
        "  docs_translate.append(Document(page_content=GoogleTranslator(source='auto', target='en').translate(doc.page_content)))\n",
        "\n",
        "db = FAISS.from_documents(docs_translate, OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "lZfpX-8nd_2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5990430-86d8-4278-f81e-4dd97c03782d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 903/1018 [18:30<02:26,  1.28s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "project_path = \"/content/drive/MyDrive\"\n",
        "\n",
        "db.save_local(os.path.join(project_path, 'UII_15_08_2023_en'))project_path = \"/content/drive/MyDrive\"\n",
        "\n",
        "db.save_local(os.path.join(project_path, 'UII_15_08_2023_en'))"
      ],
      "metadata": {
        "id": "0-XXRrl1eMxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "with open(os.path.join(project_path, 'docs_translate_new.pkl'), 'wb') as f:\n",
        "  pickle.dump(docs_translate, f, protocol = None, fix_imports = True)"
      ],
      "metadata": {
        "id": "22hh815ceR0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–∑–∞–≥—Ä—É–∑–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω–æ–π –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –±–∞–∑—ã –∏–∑ —Ñ–∞–π–ª–∞"
      ],
      "metadata": {
        "id": "FOBbcboPeVcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter, CharacterTextSplitter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/–ü—Ä–æ–µ–∫—Ç—ã/Local LLMs en\"\n",
        "\n",
        "db = FAISS.load_local(os.path.join(project_path, 'UII_15_08_2023_en'), OpenAIEmbeddings())"
      ],
      "metadata": {
        "id": "ajBPJtO8eWsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ó–∞–≥—Ä—É–∂–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã"
      ],
      "metadata": {
        "id": "InBrDqfUetiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_qw = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/ChatGPTPro–î –∑/Dz_24(21)LLM_–õ–æ–∫–∞–ª—å–Ω—ã–µ_–∫–æ–Ω—Ç—É—Ä–Ω—ã–µ_–º–æ–¥–µ–ª–∏/–≤–æ–ø—Ä–æ—Å—ã.csv\", delimiter='/n', encoding='cp1251')\n",
        "df_qw['–í–æ–ø—Ä–æ—Å'].values"
      ],
      "metadata": {
        "id": "C9SzDmGDeurT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏"
      ],
      "metadata": {
        "id": "yoBP_ejze2k_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama-2-7B-chat ggml 4"
      ],
      "metadata": {
        "id": "reGQ8SR90IoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_1.bin"
      ],
      "metadata": {
        "id": "eM4fRSMre3r0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import re\n",
        "\n",
        "n_gpu_layers = 20000  # Metal set to 1 is enough.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llama = LlamaCpp(\n",
        "    model_path=\"/content/llama-2-7b-chat.ggmlv3.q4_1.bin\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    n_ctx=4096,  # Context window\n",
        "    max_tokens=1000,  # Max tokens to generate\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# vectorstore_llama = Chroma(embedding_function=GPT4AllEmbeddings(),persist_directory=\"./chroma_db_llama\")"
      ],
      "metadata": {
        "id": "rC3wYof80Xge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<s>[INST] <<SYS>>–¢—ã –º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ —á–∞—Ç–µ –†–æ—Å—Å–∏–π—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò—Å—Å–∫—É—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ö–æ–º–ø–∞–Ω–∏—è –ø—Ä–æ–¥–∞–µ—Ç –∫—É—Ä—Å—ã –ø–æ AI.\n",
        "–£ –∫–æ–º–ø–∞–Ω–∏–∏ –µ—Å—Ç—å –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ –≤—Å–µ–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –¢–µ–±–µ –∑–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç –≤ —á–∞—Ç–µ, –¥–∞–π –µ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞,\n",
        "–æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –æ—Ç—Ä—ã–≤–∫–∏ –∏–∑ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã —á–µ–ª–æ–≤–µ–∫ –∑–∞—Ö–æ—Ç–µ–ª –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –∫—É–ø–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ. –û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\n",
        "—Ç–æ—á–Ω–æ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –µ–≥–æ –æ—Ç—Ä—ã–≤–∫–æ–≤ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ, –∫–ª–∏–µ–Ω—Ç –Ω–∏—á–µ–≥–æ –Ω–µ –¥–æ–ª–∂–µ–Ω\n",
        "–∑–Ω–∞—Ç—å –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É —Ç—ã –æ—Ç–≤–µ—á–∞–µ—à—å. –û—Ç–≤–µ—á–∞–π –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –æ–ø–∏—Ä–∞–µ—à—å—Å—è. –ï—Å–ª–∏ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, —Ç–æ –æ—Ç–≤–µ—Ç—å \"–Ø –Ω–µ –º–æ–≥—É –æ—Ç–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å\".\n",
        "<</SYS>>\n",
        "\n",
        "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞:\n",
        "{question}\n",
        "\n",
        "–î–æ–∫—É–º–µ–Ω—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –∫–ª–∏–µ–Ω—Ç—É:\n",
        "{docs}\n",
        "\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\", 'docs'])"
      ],
      "metadata": {
        "id": "ITcHxMBJ0kp-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llama)\n",
        "history = []\n",
        "\n",
        "for query in df_qw['–í–æ–ø—Ä–æ—Å'].values:\n",
        "  sim_docs = db.similarity_search(query)\n",
        "  docs = re.sub(r'\\n{2}', ' ', '\\n '.join([f'======\\n' + doc.page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n",
        "  result = llm_chain.run({'question':query, 'docs': docs})\n",
        "  history.append([str(query), GoogleTranslator(source='auto', target='ru').translate(result), str(docs)])\n",
        "pd.DataFrame(history, columns=['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç']).to_csv(os.path.join(project_path, 'Llama-2-7B-Chat-GGML-4' + '.csv'), index=False)"
      ],
      "metadata": {
        "id": "zHsFAU870rS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StableBeluga 2 13B gguf"
      ],
      "metadata": {
        "id": "fEvN9MKq01CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/StableBeluga-13B-GGUF/resolve/main/stablebeluga-13b.Q4_0.gguf"
      ],
      "metadata": {
        "id": "jTzscz-m02Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from deep_translator import GoogleTranslator\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "n_gpu_layers = 1000  # Metal set to 1 is enough.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llama = LlamaCpp(\n",
        "    model_path=\"/content/stablebeluga-13b.Q4_0.gguf\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    temperature=0,\n",
        "    n_ctx=4096,  # Context window\n",
        "    max_tokens=1000,  # Max tokens to generate\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# vectorstore_llama = Chroma(embedding_function=GPT4AllEmbeddings(),persist_directory=\"./chroma_db_llama\")"
      ],
      "metadata": {
        "id": "47BNZ39P0_9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"### System: \\n–¢—ã –º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ —á–∞—Ç–µ –†–æ—Å—Å–∏–π—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò—Å—Å–∫—É—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ö–æ–º–ø–∞–Ω–∏—è –ø—Ä–æ–¥–∞–µ—Ç –∫—É—Ä—Å—ã –ø–æ AI.\n",
        "–£ –∫–æ–º–ø–∞–Ω–∏–∏ –µ—Å—Ç—å –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ –≤—Å–µ–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –¢–µ–±–µ –∑–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç –≤ —á–∞—Ç–µ, –¥–∞–π –µ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞,\n",
        "–æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –æ—Ç—Ä—ã–≤–∫–∏ –∏–∑ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã —á–µ–ª–æ–≤–µ–∫ –∑–∞—Ö–æ—Ç–µ–ª –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –∫—É–ø–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ. –û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\n",
        "—Ç–æ—á–Ω–æ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –µ–≥–æ –æ—Ç—Ä—ã–≤–∫–æ–≤ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ, –∫–ª–∏–µ–Ω—Ç –Ω–∏—á–µ–≥–æ –Ω–µ –¥–æ–ª–∂–µ–Ω\n",
        "–∑–Ω–∞—Ç—å –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É —Ç—ã –æ—Ç–≤–µ—á–∞–µ—à—å. –û—Ç–≤–µ—á–∞–π –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –æ–ø–∏—Ä–∞–µ—à—å—Å—è. –ï—Å–ª–∏ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, —Ç–æ –æ—Ç–≤–µ—Ç—å \"–Ø –Ω–µ –º–æ–≥—É –æ—Ç–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å\".\\n\\n\n",
        "### User: \\n–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞:\n",
        "{question}\\n\n",
        "–î–æ–∫—É–º–µ–Ω—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –∫–ª–∏–µ–Ω—Ç—É:\n",
        "{docs}\\n\\n\n",
        "### Assistant:\\n\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\", 'docs'])"
      ],
      "metadata": {
        "id": "w9yPqFas1E3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llama)\n",
        "history = []\n",
        "\n",
        "for query in df_qw['–í–æ–ø—Ä–æ—Å'].values:\n",
        "  sim_docs = db.similarity_search(query)\n",
        "  docs = re.sub(r'\\n{2}', ' ', '\\n '.join([f'======\\n' + doc.page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n",
        "  result = llm_chain.run({'question':query, 'docs': docs})\n",
        "  history.append([str(query), result, str(docs)])\n",
        "pd.DataFrame(history, columns=['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç']).to_csv(os.path.join(project_path, 'StableBeluga-2-13B-GGUF-4' + '.csv'), index=False)"
      ],
      "metadata": {
        "id": "kpFYuiV71J-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saiga2_7B_8_gguf"
      ],
      "metadata": {
        "id": "yAlJSb1w1iHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import fire\n",
        "from llama_cpp import Llama\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"–¢—ã –º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ —á–∞—Ç–µ –†–æ—Å—Å–∏–π—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò—Å—Å–∫—É—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ö–æ–º–ø–∞–Ω–∏—è –ø—Ä–æ–¥–∞–µ—Ç –∫—É—Ä—Å—ã –ø–æ AI.\n",
        "–£ –∫–æ–º–ø–∞–Ω–∏–∏ –µ—Å—Ç—å –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ –≤—Å–µ–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –¢–µ–±–µ –∑–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç –≤ —á–∞—Ç–µ, –¥–∞–π –µ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞,\n",
        "–æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –æ—Ç—Ä—ã–≤–∫–∏ –∏–∑ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã —á–µ–ª–æ–≤–µ–∫ –∑–∞—Ö–æ—Ç–µ–ª –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –∫—É–ø–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ. –û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\n",
        "—Ç–æ—á–Ω–æ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –µ–≥–æ –æ—Ç—Ä—ã–≤–∫–æ–≤ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ, –∫–ª–∏–µ–Ω—Ç –Ω–∏—á–µ–≥–æ –Ω–µ –¥–æ–ª–∂–µ–Ω\n",
        "–∑–Ω–∞—Ç—å –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É —Ç—ã –æ—Ç–≤–µ—á–∞–µ—à—å. –û—Ç–≤–µ—á–∞–π –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –æ–ø–∏—Ä–∞–µ—à—å—Å—è. –ï—Å–ª–∏ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, —Ç–æ –æ—Ç–≤–µ—Ç—å \"–Ø –Ω–µ –º–æ–≥—É –æ—Ç–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å\".\n",
        "\"\"\"\n",
        "SYSTEM_TOKEN = 1788\n",
        "USER_TOKEN = 1404\n",
        "BOT_TOKEN = 9225\n",
        "LINEBREAK_TOKEN = 13\n",
        "\n",
        "ROLE_TOKENS = {\n",
        "    \"user\": USER_TOKEN,\n",
        "    \"bot\": BOT_TOKEN,\n",
        "    \"system\": SYSTEM_TOKEN\n",
        "}\n",
        "\n",
        "def get_message_tokens(model, role, content):\n",
        "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
        "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
        "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
        "    message_tokens.append(model.token_eos())\n",
        "    return message_tokens\n",
        "\n",
        "\n",
        "def get_system_tokens(model):\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": SYSTEM_PROMPT\n",
        "    }\n",
        "    return get_message_tokens(model, **system_message)\n",
        "\n",
        "\n",
        "def interact(\n",
        "    model,\n",
        "    query,\n",
        "    tokens,\n",
        "    top_k=30,\n",
        "    top_p=0.9,\n",
        "    temperature=0.2,\n",
        "    repeat_penalty=1.1\n",
        "):\n",
        "\n",
        "    answer = []\n",
        "\n",
        "    while True:\n",
        "        user_message = f\"User: {query}\"\n",
        "        message_tokens = get_message_tokens(model=model, role=\"user\", content=user_message)\n",
        "        role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
        "        tokens += message_tokens + role_tokens\n",
        "        generator = model.generate(\n",
        "            tokens,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            temp=temperature,\n",
        "            repeat_penalty=repeat_penalty\n",
        "        )\n",
        "        for token in generator:\n",
        "            token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n",
        "            tokens.append(token)\n",
        "            answer.append(token_str)\n",
        "            if token == model.token_eos():\n",
        "                return ''.join(answer)\n",
        "                break\n",
        "\n",
        "            print(token_str, end=\"\", flush=True)\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "model = Llama(\n",
        "    model_path='/content/model-q8_0.gguf',\n",
        "    n_ctx=4096,\n",
        "    n_parts=1,\n",
        "    n_gpu_layers=150,\n",
        "    n_batch=512,\n",
        "    n_gqa=8,\n",
        ")\n",
        "\n",
        "system_tokens = get_system_tokens(model)\n",
        "tokens = system_tokens\n",
        "model.eval(tokens)"
      ],
      "metadata": {
        "id": "E3g7CTUg1fgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "history = []\n",
        "\n",
        "for query in df_qw['–í–æ–ø—Ä–æ—Å'].values:\n",
        "  sim_docs = db.similarity_search(query)\n",
        "  docs = re.sub(r'\\n{2}', ' ', '\\n '.join([f'======\\n' + doc.page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n",
        "  template = f\"\"\"\n",
        "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞:\n",
        "{query}\n",
        "\n",
        "–î–æ–∫—É–º–µ–Ω—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –∫–ª–∏–µ–Ω—Ç—É:\n",
        "{docs}\n",
        "\"\"\"\n",
        "  system_tokens = get_system_tokens(model)\n",
        "  tokens = system_tokens\n",
        "  model.eval(tokens)\n",
        "  result = interact(model, query=template, tokens=tokens)\n",
        "  history.append([str(query), result, str(docs)])\n",
        "pd.DataFrame(history, columns=['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç']).to_csv(os.path.join(project_path, 'Saiga2_7b_GGUF-8' + '.csv'), index=False)"
      ],
      "metadata": {
        "id": "IHAEsXZ31tLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saiga2_13B_4_gguf"
      ],
      "metadata": {
        "id": "f54mrEYv15dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import fire\n",
        "from llama_cpp import Llama\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"–¢—ã –º–µ–Ω–µ–¥–∂–µ—Ä –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –≤ —á–∞—Ç–µ –†–æ—Å—Å–∏–π—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç –ò—Å—Å–∫—É—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–∞. –ö–æ–º–ø–∞–Ω–∏—è –ø—Ä–æ–¥–∞–µ—Ç –∫—É—Ä—Å—ã –ø–æ AI.\n",
        "–£ –∫–æ–º–ø–∞–Ω–∏–∏ –µ—Å—Ç—å –±–æ–ª—å—à–æ–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ –≤—Å–µ–º–∏ –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏ –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –¢–µ–±–µ –∑–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç –≤ —á–∞—Ç–µ, –¥–∞–π –µ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ —è–∑—ã–∫–µ –æ—Ä–∏–≥–∏–Ω–∞–ª–∞,\n",
        "–æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –æ—Ç—Ä—ã–≤–∫–∏ –∏–∑ —ç—Ç–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –ø–æ—Å—Ç–∞—Ä–∞–π—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å —Ç–∞–∫, —á—Ç–æ–±—ã —á–µ–ª–æ–≤–µ–∫ –∑–∞—Ö–æ—Ç–µ–ª –ø–æ—Å–ª–µ –æ—Ç–≤–µ—Ç–∞ –∫—É–ø–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ. –û—Ç–≤–µ—á–∞–π –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ\n",
        "—Ç–æ—á–Ω–æ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É, –Ω–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –ù–∏–∫–æ–≥–¥–∞ –Ω–µ —Å—Å—ã–ª–∞–π—Å—è –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è –µ–≥–æ –æ—Ç—Ä—ã–≤–∫–æ–≤ –ø—Ä–∏ –æ—Ç–≤–µ—Ç–µ, –∫–ª–∏–µ–Ω—Ç –Ω–∏—á–µ–≥–æ –Ω–µ –¥–æ–ª–∂–µ–Ω\n",
        "–∑–Ω–∞—Ç—å –æ –¥–æ–∫—É–º–µ–Ω—Ç–µ, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É —Ç—ã –æ—Ç–≤–µ—á–∞–µ—à—å. –û—Ç–≤–µ—á–∞–π –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞ –±–µ–∑ —Å—Å—ã–ª–æ–∫ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –æ–ø–∏—Ä–∞–µ—à—å—Å—è. –ï—Å–ª–∏ —Ç—ã –Ω–µ –∑–Ω–∞–µ—à—å –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ –µ–≥–æ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ, —Ç–æ –æ—Ç–≤–µ—Ç—å \"–Ø –Ω–µ –º–æ–≥—É –æ—Ç–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å\".\n",
        "\"\"\"\n",
        "SYSTEM_TOKEN = 1788\n",
        "USER_TOKEN = 1404\n",
        "BOT_TOKEN = 9225\n",
        "LINEBREAK_TOKEN = 13\n",
        "\n",
        "ROLE_TOKENS = {\n",
        "    \"user\": USER_TOKEN,\n",
        "    \"bot\": BOT_TOKEN,\n",
        "    \"system\": SYSTEM_TOKEN\n",
        "}\n",
        "\n",
        "def get_message_tokens(model, role, content):\n",
        "    message_tokens = model.tokenize(content.encode(\"utf-8\"))\n",
        "    message_tokens.insert(1, ROLE_TOKENS[role])\n",
        "    message_tokens.insert(2, LINEBREAK_TOKEN)\n",
        "    message_tokens.append(model.token_eos())\n",
        "    return message_tokens\n",
        "\n",
        "\n",
        "def get_system_tokens(model):\n",
        "    system_message = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": SYSTEM_PROMPT\n",
        "    }\n",
        "    return get_message_tokens(model, **system_message)\n",
        "\n",
        "\n",
        "def interact(\n",
        "    model,\n",
        "    query,\n",
        "    tokens,\n",
        "    top_k=30,\n",
        "    top_p=0.9,\n",
        "    temperature=0.2,\n",
        "    repeat_penalty=1.1\n",
        "):\n",
        "\n",
        "    answer = []\n",
        "\n",
        "    while True:\n",
        "        user_message = f\"User: {query}\"\n",
        "        message_tokens = get_message_tokens(model=model, role=\"user\", content=user_message)\n",
        "        role_tokens = [model.token_bos(), BOT_TOKEN, LINEBREAK_TOKEN]\n",
        "        tokens += message_tokens + role_tokens\n",
        "        generator = model.generate(\n",
        "            tokens,\n",
        "            top_k=top_k,\n",
        "            top_p=top_p,\n",
        "            temp=temperature,\n",
        "            repeat_penalty=repeat_penalty\n",
        "        )\n",
        "        for token in generator:\n",
        "            token_str = model.detokenize([token]).decode(\"utf-8\", errors=\"ignore\")\n",
        "            tokens.append(token)\n",
        "            answer.append(token_str)\n",
        "            if token == model.token_eos():\n",
        "                return ''.join(answer)\n",
        "                break\n",
        "\n",
        "            print(token_str, end=\"\", flush=True)\n",
        "        print()\n",
        "\n",
        "\n",
        "\n",
        "model = Llama(\n",
        "    model_path='/content/model-q4_K.gguf',\n",
        "    n_ctx=4096,\n",
        "    n_parts=1,\n",
        "    n_gpu_layers=150,\n",
        "    n_batch=512,\n",
        "    n_gqa=8,\n",
        ")\n",
        "\n",
        "system_tokens = get_system_tokens(model)\n",
        "tokens = system_tokens\n",
        "model.eval(tokens)"
      ],
      "metadata": {
        "id": "p-yc876G13X9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "history = []\n",
        "\n",
        "for query in df_qw['–í–æ–ø—Ä–æ—Å'].values:\n",
        "  sim_docs = db.similarity_search(query)\n",
        "  docs = re.sub(r'\\n{2}', ' ', '\\n '.join([f'======\\n' + doc.page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n",
        "  template = f\"\"\"\n",
        "–í–æ–ø—Ä–æ—Å –∫–ª–∏–µ–Ω—Ç–∞:\n",
        "{query}\n",
        "\n",
        "–î–æ–∫—É–º–µ–Ω—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –∫–ª–∏–µ–Ω—Ç—É:\n",
        "{docs}\n",
        "\"\"\"\n",
        "  system_tokens = get_system_tokens(model)\n",
        "  tokens = system_tokens\n",
        "  model.eval(tokens)\n",
        "  result = interact(model, query=template, tokens=tokens)\n",
        "  history.append([str(query), result, str(docs)])\n",
        "pd.DataFrame(history, columns=['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç']).to_csv(os.path.join(project_path, 'Saiga2_13b_GGUF-4' + '.csv'), index=False)"
      ],
      "metadata": {
        "id": "Al0K4XG32FAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É"
      ],
      "metadata": {
        "id": "j6bTeKcK2Utx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/–ü—Ä–æ–µ–∫—Ç—ã/LLMs_local\"\n",
        "files = os.listdir(project_path)\n",
        "files"
      ],
      "metadata": {
        "id": "t__niKxj2Qxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.DataFrame(columns=['–í–æ–ø—Ä–æ—Å']+files)\n",
        "for count, file in enumerate(files):\n",
        "  if file not in ['score_result.csv', 'score_result.xlsx', \".ipynb_checkpoints\"]:\n",
        "    df = pd.read_csv(os.path.join(project_path, file))\n",
        "    if count == 0:\n",
        "      df_all['–í–æ–ø—Ä–æ—Å'] = df['–í–æ–ø—Ä–æ—Å']\n",
        "      df_all[file] = df['–û—Ç–≤–µ—Ç']\n",
        "    else:\n",
        "      df_all[file] = df['–û—Ç–≤–µ—Ç']\n",
        "\n",
        "df_all.to_csv(os.path.join(project_path, 'score_result'+'.csv'), index=False)\n",
        "df_all.to_excel(os.path.join(project_path, 'score_result.xlsx'), index=False)"
      ],
      "metadata": {
        "id": "WfmITvuJ2eao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/–ü—Ä–æ–µ–∫—Ç—ã/LLMs_local\"\n",
        "files = [file for file in os.listdir(project_path) if file not in ['score_result.csv', 'score_result.xlsx', \".ipynb_checkpoints\", 'Platypus2-70b-instruct-ggml-4.csv',\n",
        "\n",
        " 'Llama-2-7B-Chat-GGML-4.csv',\n",
        " 'StableBeluga-2-13B-GGML-4.csv',\n",
        "\n",
        " 'OpenAI_chatgpt.csv',\n",
        " 'OpenAI_gpt4.csv']]\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame —Å –Ω—É–∂–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π\n",
        "df_all = pd.DataFrame(columns=['–í–æ–ø—Ä–æ—Å', '–ú–æ–¥–µ–ª—å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç'])\n",
        "\n",
        "for file in files:\n",
        "    df = pd.read_csv(os.path.join(project_path, file))\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º \"–í–æ–ø—Ä–æ—Å\", \"–û—Ç–≤–µ—Ç\" –∏ \"–ö–æ–Ω—Ç–µ–∫—Å—Ç\"\n",
        "    df_merged = df[['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç']]\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ (–º–æ–¥–µ–ª–∏) –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É\n",
        "    df_merged['–ú–æ–¥–µ–ª—å'] = file\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫ –æ–±—â–µ–π —Ç–∞–±–ª–∏—Ü–µ\n",
        "    df_all = pd.concat([df_all, df_merged], ignore_index=True)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â—É—é —Ç–∞–±–ª–∏—Ü—É –≤ CSV –∏ XLSX\n",
        "df_all.to_csv(os.path.join(project_path, 'score_result_add.csv'), index=False)\n",
        "df_all.to_excel(os.path.join(project_path, 'score_result_add.xlsx'), index=False)"
      ],
      "metadata": {
        "id": "IcnBk8e72lZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º (—Å –ø–µ—Ä–µ–≤–æ–¥–æ–º)"
      ],
      "metadata": {
        "id": "I6ZsP7DP3KHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Llama-2-7B-chat ggml 4"
      ],
      "metadata": {
        "id": "P6lK-1eM6uKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-7B-chat-GGML/resolve/main/llama-2-7b-chat.ggmlv3.q4_1.bin"
      ],
      "metadata": {
        "id": "3zxywIi638BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "import re\n",
        "\n",
        "n_gpu_layers = 20000  # Metal set to 1 is enough.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llama = LlamaCpp(\n",
        "    model_path=\"/content/llama-2-7b-chat.ggmlv3.q4_1.bin\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    n_ctx=4096,  # Context window\n",
        "    max_tokens=1000,  # Max tokens to generate\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# vectorstore_llama = Chroma(embedding_function=GPT4AllEmbeddings(),persist_directory=\"./chroma_db_llama\")"
      ],
      "metadata": {
        "id": "LCmFPMWJ4Cqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"<s>[INST] <<SYS>>You are the chat support manager for the Russian company University of Artificial Intelligence. The company sells courses on AI.\n",
        "The company has a large document with all the materials about the company's products in Russian. A client asks you a question in chat, give him an answer Based on excerpts from this document, try to answer in such a way that the person will want to buy training after answering. Answer as much as possible\n",
        "exactly according to the document, do not invent anything on your own. Never refer to the title of a document or the title of its passages when answering; the client does not owe anything\n",
        "know about the document for which you are responding. Answer in the first person without citing the sources you rely on. If you do not know the answer or it is not in the document, then answer ‚ÄúI cannot answer this question.‚Äù\n",
        "<</SYS>>\n",
        "\n",
        "Client Question:\n",
        "{question}\n",
        "\n",
        "Document with information to respond to the client:\n",
        "{docs}\n",
        "\n",
        "[/INST]\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\", 'docs'])"
      ],
      "metadata": {
        "id": "ecTW9gvT4Iyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llama)\n",
        "history = []\n",
        "\n",
        "for query in df_qw['–í–æ–ø—Ä–æ—Å'].values:\n",
        "  sim_docs = db.similarity_search(GoogleTranslator(source='auto', target='en').translate(query))\n",
        "  docs = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\"======\"\\n' + doc.page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n",
        "  result = llm_chain.run({'question':GoogleTranslator(source='auto', target='en').translate(query), 'docs': docs})\n",
        "  history.append([str(query), GoogleTranslator(source='auto', target='ru').translate(result),\n",
        "                  GoogleTranslator(source='auto', target='ru').translate(docs)])\n",
        "pd.DataFrame(history, columns=['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç']).to_csv(os.path.join(project_path, 'Llama-2-7B-Chat-GGML-4-en' + '.csv'), index=False)"
      ],
      "metadata": {
        "id": "7nIkwmRx4N4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "StableBeluga 2 13B ggml"
      ],
      "metadata": {
        "id": "yanX5Ms35WuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/StableBeluga-13B-GGML/resolve/main/stablebeluga-13b.ggmlv3.q4_1.bin"
      ],
      "metadata": {
        "id": "TpoOGVB35Yl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "from langchain.embeddings import GPT4AllEmbeddings\n",
        "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.document_loaders import TextLoader\n",
        "from deep_translator import GoogleTranslator\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "n_gpu_layers = 4000  # Metal set to 1 is enough.\n",
        "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "llama = LlamaCpp(\n",
        "    model_path=\"/content/stablebeluga-13b.ggmlv3.q4_1.bin\",\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_batch=n_batch,\n",
        "    temperature=0,\n",
        "    n_ctx=4096,  # Context window\n",
        "    max_tokens=1000,  # Max tokens to generate\n",
        "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# vectorstore_llama = Chroma(embedding_function=GPT4AllEmbeddings(),persist_directory=\"./chroma_db_llama\")"
      ],
      "metadata": {
        "id": "J2-kgcWs5jLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"### System: \\nYou are the chat support manager of the Russian company University of Artificial Intelligence. The company sells courses on AI.\n",
        "The company has a large document with all the materials about the company's products in Russian. A client asks you a question in a chat, give him an answer based on excerpts from this document, try to answer in such a way that the person wants to buy training after answering. Answer as much as possible\n",
        "exactly according to the document, do not invent anything on your own. Never refer to the title of a document or the title of its passages when answering; the client does not owe anything\n",
        "know about the document for which you are responding. Answer in the first person without citing the sources you rely on. If you do not know the answer or it is not in the document, then answer ‚ÄúI cannot answer this question.‚Äù\\n\\n\n",
        "### User: \\nClient question:\n",
        "{question}\\n\n",
        "Document with information to respond to the client:\n",
        "{docs}\\n\\n\n",
        "### Assistant:\\n\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\", 'docs'])"
      ],
      "metadata": {
        "id": "GzH6yeDl5oZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from deep_translator import GoogleTranslator\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llama)\n",
        "history = []\n",
        "\n",
        "for query in df_qw['–í–æ–ø—Ä–æ—Å'].values:\n",
        "  sim_docs = db.similarity_search(GoogleTranslator(source='auto', target='en').translate(query))\n",
        "  docs = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\"======\"\\n' + doc.page_content + '\\n' for i, doc in enumerate(sim_docs)]))\n",
        "  result = llm_chain.run({'question':GoogleTranslator(source='auto', target='en').translate(query), 'docs': docs})\n",
        "  history.append([str(query), GoogleTranslator(source='auto', target='ru').translate(result),\n",
        "                  GoogleTranslator(source='auto', target='ru').translate(docs)])\n",
        "pd.DataFrame(history, columns=['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç']).to_csv(os.path.join(project_path, 'StableBeluga-2-13B-GGML-4-en' + '.csv'), index=False)"
      ],
      "metadata": {
        "id": "wALyd4UX5tQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É"
      ],
      "metadata": {
        "id": "CWrd98qU8IUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/–ü—Ä–æ–µ–∫—Ç—ã/Local LLMs en\"\n",
        "files = os.listdir(project_path)\n",
        "files"
      ],
      "metadata": {
        "id": "PDniXBGA8OEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [file for file in os.listdir(project_path) if file not in ['UII_15_08_2023_en',\n",
        " 'docs_translate_new.pkl',\n",
        " 'Falcon_180B_chat_gguf_2.csv',\n",
        " 'Falcon_180B_chat_gguf_2.gsheet',\n",
        " ]]\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π DataFrame —Å –Ω—É–∂–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π\n",
        "df_all = pd.DataFrame(columns=['–í–æ–ø—Ä–æ—Å', '–ú–æ–¥–µ–ª—å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç'])\n",
        "\n",
        "for file in files:\n",
        "    df = pd.read_csv(os.path.join(project_path, file))\n",
        "    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º \"–í–æ–ø—Ä–æ—Å\", \"–û—Ç–≤–µ—Ç\" –∏ \"–ö–æ–Ω—Ç–µ–∫—Å—Ç\"\n",
        "    df_merged = df[['–í–æ–ø—Ä–æ—Å', '–û—Ç–≤–µ—Ç', '–ö–æ–Ω—Ç–µ–∫—Å—Ç']]\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ (–º–æ–¥–µ–ª–∏) –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É\n",
        "    df_merged['–ú–æ–¥–µ–ª—å'] = file\n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –∫ –æ–±—â–µ–π —Ç–∞–±–ª–∏—Ü–µ\n",
        "    df_all = pd.concat([df_all, df_merged], ignore_index=True)\n",
        "\n",
        "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â—É—é —Ç–∞–±–ª–∏—Ü—É –≤ CSV –∏ XLSX\n",
        "df_all.to_csv(os.path.join(project_path, 'score_result_eng.csv'), index=False)\n",
        "df_all.to_excel(os.path.join(project_path, 'score_result_eng.xlsx'), index=False)"
      ],
      "metadata": {
        "id": "58FvEpTn8YTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "–í—ã–≤–æ–¥—ã"
      ],
      "metadata": {
        "id": "6dcgARI_8jeo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vZR_zc7F8ixd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}